{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10534b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. TOKENIZER (BPE) ---\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=1024):\n",
    "        self.vocab_size, self.merges, self.vocab = vocab_size, {}, {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "    def train(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        for i in range(self.vocab_size - 256):\n",
    "            stats = {}\n",
    "            for pair in zip(tokens, tokens[1:]): stats[pair] = stats.get(pair, 0) + 1\n",
    "            if not stats: break\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            self.merges[pair], self.vocab[idx] = idx, self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "            new_tokens, j = [], 0\n",
    "            while j < len(tokens):\n",
    "                if j < len(tokens)-1 and (tokens[j], tokens[j+1]) == pair:\n",
    "                    new_tokens.append(idx); j += 2\n",
    "                else: new_tokens.append(tokens[j]); j += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) >= 2:\n",
    "            stats = {p: self.merges[p] for p in zip(tokens, tokens[1:]) if p in self.merges}\n",
    "            if not stats: break\n",
    "            pair = min(stats.keys(), key=lambda p: self.merges[p])\n",
    "            idx, new_tokens, j = self.merges[pair], [], 0\n",
    "            while j < len(tokens):\n",
    "                if j < len(tokens)-1 and (tokens[j], tokens[j+1]) == pair:\n",
    "                    new_tokens.append(idx); j += 2\n",
    "                else: new_tokens.append(tokens[j]); j += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return b\"\".join(self.vocab.get(i, b\"<?>\") for i in ids).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "# --- 2. THE SOVEREIGN GPT CLASS ---\n",
    "class SovereignGPT:\n",
    "    def __init__(self, v_size, emb, layers, b_size, n_experts, cycles):\n",
    "        self.v_size, self.emb, self.layers, self.b_size = v_size, emb, layers, b_size\n",
    "        self.n_experts = n_experts\n",
    "        self.dt = 0.5\n",
    "        self.cycles = cycles\n",
    "        \n",
    "        self.W = {}\n",
    "        self.init_weights()\n",
    "        # Adam buffers\n",
    "        self.m = {k: np.zeros_like(v, dtype=np.float32) for k, v in self.W.items()}\n",
    "        self.v = {k: np.zeros_like(v, dtype=np.float32) for k, v in self.W.items()}\n",
    "        self.t = 0\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Embedding\n",
    "        self.W['E'] = np.random.randn(self.v_size, self.emb) * np.sqrt(1/self.emb)\n",
    "        # Router (Projection to experts)\n",
    "        self.W['router'] = np.random.randn(self.emb, self.n_experts) * 0.02\n",
    "        \n",
    "        hidden = int(8/3 * self.emb) \n",
    "        \n",
    "        for l in range(self.layers):\n",
    "            self.W[f'W_kv_up_l{l}'] = np.random.randn(self.emb, self.emb * 2) * np.sqrt(1/self.emb)\n",
    "            for e in range(self.n_experts):\n",
    "                self.W[f'Wq_e{e}_l{l}'] = np.random.randn(self.emb, self.emb) * 0.02\n",
    "                self.W[f'Wo_e{e}_l{l}'] = np.random.randn(self.emb, self.emb) * 0.02\n",
    "                self.W[f'tau_e{e}_l{l}'] = np.exp(np.random.uniform(-1, 1, (1, self.emb)))\n",
    "            \n",
    "            # THE ZNN / SWIGLU WEIGHTS (The memory filters)\n",
    "            self.W[f'W_gate_l{l}'] = np.random.randn(self.emb, hidden) * np.sqrt(2/self.emb)\n",
    "            self.W[f'W_up_l{l}'] = np.random.randn(self.emb, hidden) * np.sqrt(2/self.emb)\n",
    "            self.W[f'W_down_l{l}'] = np.random.randn(hidden, self.emb) * np.sqrt(2/hidden)\n",
    "\n",
    "            # 1. Predictive State Head (Predicting the NEXT hidden state vector)\n",
    "            self.W['W_pred_state'] = np.random.randn(self.emb, self.emb) * 0.02\n",
    "\n",
    "            # 2. Multi-Token Heads (Guessing the next TWO tokens at once)\n",
    "            self.W['W_head_t1'] = np.random.randn(self.emb, self.v_size) * 0.02\n",
    "            self.W['W_head_t2'] = np.random.randn(self.emb, self.v_size) * 0.02\n",
    "\n",
    "            # 3. Recursive Confidence Gate (Determines when to stop looping)\n",
    "            self.W['W_recur_gate'] = np.random.randn(self.emb, 1) * 0.02\n",
    "\n",
    "    # --- Comfort Functions from Old Code ---\n",
    "    def rms_norm(self, x):\n",
    "        return x * (np.mean(x**2, axis=-1, keepdims=True) + 1e-6)**-0.5\n",
    "\n",
    "    def softmax(self, x):\n",
    "        x_max = np.max(x, axis=-1, keepdims=True)\n",
    "        e_x = np.exp(x - x_max)\n",
    "        return e_x / (e_x.sum(axis=-1, keepdims=True) + 1e-10)\n",
    "\n",
    "    def get_batch(self, data, batch_size):\n",
    "        ix = np.random.randint(0, len(data) - self.b_size, batch_size)\n",
    "        x = np.array([data[i:i+self.b_size] for i in ix])\n",
    "        y = np.array([data[i+1:i+self.b_size+1] for i in ix])\n",
    "        return x, y\n",
    "\n",
    "    def forward(self, idx, return_cache=False):\n",
    "        B, T = idx.shape\n",
    "        h = self.rms_norm(self.W['E'][idx])\n",
    "        cache = {'h_0': h.copy()}\n",
    "        \n",
    "        # 1. Create a single causal mask for one sequence\n",
    "        causal_mask = np.triu(np.ones((T, T)), k=1) * -1e9\n",
    "        \n",
    "        for l in range(self.layers):\n",
    "            for r in range(self.cycles):\n",
    "                h_in = h.copy()\n",
    "                h_flat = h.reshape(-1, self.emb) # (512, 384)\n",
    "\n",
    "                route_logits = h_flat @ self.W['router']\n",
    "                expert_idx = np.argmax(route_logits, axis=-1)\n",
    "                cache[f'expert_idx_l{l}'] = expert_idx\n",
    "                \n",
    "                # 1. MLA: Get compressed KV for the whole batch\n",
    "                kv = h @ self.W[f'W_kv_up_l{l}']\n",
    "                k, v = np.split(kv, 2, axis=-1) # (B, T, Emb)\n",
    "\n",
    "                # 2. Query for the tokens\n",
    "                q = h_flat @ self.W[f'Wq_e0_l{l}'] # (B*T, Emb)\n",
    "                q = q.reshape(B, T, self.emb)\n",
    "\n",
    "                # 3. Efficient Batch Attention (The \"Streaming\" part)\n",
    "                # We do this for all 512 tokens at once\n",
    "                attn = (q @ k.transpose(0, 2, 1)) / np.sqrt(self.emb)\n",
    "                attn += causal_mask # Apply the 2026 \"Streaming\" mask\n",
    "                sm = self.softmax(attn)\n",
    "                context = (sm @ v) @ self.W[f'Wo_e0_l{l}'] # (B, T, Emb)\n",
    "\n",
    "                context_flat = context.reshape(-1, self.emb) # Ensure this is (512, 384)\n",
    "                cache[f'context_l{l}'] = context_flat\n",
    "\n",
    "                # 4. Liquid & Filter Logic\n",
    "                dh_dt = (-h_flat + context_flat) / self.W[f'tau_e0_l{l}']\n",
    "                h_liquid = h_flat + ((-h_flat + context_flat) / self.W[f'tau_e0_l{l}']) * self.dt\n",
    "                \n",
    "                # 5. ZNN/SwiGLU Nudge\n",
    "                g = h_flat @ self.W[f'W_gate_l{l}']\n",
    "                gate = g * (1 / (1 + np.exp(-g)))\n",
    "                z = h_liquid @ self.W[f'W_up_l{l}']\n",
    "                h_filtered = (z * gate) @ self.W[f'W_down_l{l}'] \n",
    "                h = np.tanh(h_in + h_filtered.reshape(B, T, self.emb))\n",
    "\n",
    "                confidence = 1 / (1 + np.exp(-(h @ self.W['W_recur_gate'])))\n",
    "                if np.mean(confidence) > 0.8: break\n",
    "                \n",
    "                # --- THE FIX: Uniform Caching ---\n",
    "            cache[f'h_{l+1}'] = h.copy()\n",
    "\n",
    "            # --- MULTI-TOKEN & PREDICTIVE HEADS ---\n",
    "            # 1. Standard Head (T+1)\n",
    "            logits = h @ self.W['W_head_t1']\n",
    "            # 2. Future Head (T+2)\n",
    "            logits_t2 = h @ self.W['W_head_t2']\n",
    "            # 3. Predictive State (Dreaming of the next vector)\n",
    "            pred_h_next = h @ self.W['W_pred_state']\n",
    "            \n",
    "            cache['pred_h_next'] = pred_h_next\n",
    "            cache['logits_t2'] = logits_t2\n",
    "\n",
    "            cache[f'context_l{l}'] = context_flat # No list, just the matrix\n",
    "            cache[f'h_liq_l{l}'] = h_liquid\n",
    "            cache[f'gate_l{l}'] = gate\n",
    "            cache[f'z_l{l}'] = z\n",
    "                \n",
    "        if return_cache: return logits, cache\n",
    "        return logits\n",
    "\n",
    "    def backward(self, targets, logits, cache):\n",
    "        B, T, V = logits.shape\n",
    "        probs = self.softmax(logits)\n",
    "\n",
    "        # --- THE FOCAL KNOB ---\n",
    "        gamma = 2.0 # Higher = more focus on \"hard\" tokens\n",
    "        target_indices = np.arange(B*T)\n",
    "        flat_targets = targets.flatten()\n",
    "        \n",
    "        # Get probability the model gave to the CORRECT token\n",
    "        pt = probs.reshape(-1, V)[target_indices, flat_targets]\n",
    "        # Focal weight: (1-p)^gamma. If p is 0.99 (easy), weight is tiny.\n",
    "        f_weight = (1 - pt) ** gamma\n",
    "        \n",
    "        d_logits = probs.copy()\n",
    "        d_logits.reshape(-1, V)[target_indices, flat_targets] -= 1\n",
    "        # Apply the focal nudge\n",
    "        d_logits *= f_weight.reshape(-1, 1)\n",
    "        d_logits /= (B * T)\n",
    "        \n",
    "        grads = {k: np.zeros_like(v) for k, v in self.W.items()}\n",
    "        grads['E'] = (d_logits.transpose(0, 2, 1) @ cache[f'h_{self.layers}']).sum(0)\n",
    "        \n",
    "        probs_t2 = self.softmax(cache['logits_t2'])\n",
    "        # Target for T2 is the word AFTER targets (targets[:, 1:])\n",
    "        d_logits_t2 = np.zeros_like(probs_t2)\n",
    "        # We can only predict T+2 for the first T-1 tokens\n",
    "        t2_targets = targets[:, 1:].flatten()\n",
    "        d_logits_t2[:, :-1].reshape(-1, V)[np.arange(B*(T-1)), t2_targets] -= 1\n",
    "        d_logits_t2 = (probs_t2 + d_logits_t2) / (B * T)\n",
    "        \n",
    "        # Update Multi-token weights\n",
    "        grads['W_head_t1'] = cache[f'h_{self.layers}'].transpose(0, 2, 1).reshape(self.emb, -1) @ d_logits.reshape(-1, V)\n",
    "        grads['W_head_t2'] = cache[f'h_{self.layers}'].transpose(0, 2, 1).reshape(self.emb, -1) @ d_logits_t2.reshape(-1, V)\n",
    "\n",
    "        # 3. PREDICTIVE STATE GRADIENT (Hidden State dreaming)\n",
    "        # Error = Pred_h_next[T] - Actual_h_final[T+1]\n",
    "        h_final = cache[f'h_{self.layers}']\n",
    "        d_pred_h = np.zeros_like(h_final)\n",
    "        d_pred_h[:, :-1] = cache['pred_h_next'][:, :-1] - h_final[:, 1:]\n",
    "        grads['W_pred_state'] = h_final.transpose(0, 2, 1).reshape(self.emb, -1) @ d_pred_h.reshape(-1, self.emb)\n",
    "\n",
    "        # 4. RECURSIVE GRADIENT (Initial dh signal)\n",
    "        # The total error flowing back into the layers is a sum of all heads\n",
    "        dh = (d_logits @ self.W['W_head_t1'].T) + (d_logits_t2 @ self.W['W_head_t2'].T) + (d_pred_h @ self.W['W_pred_state'].T)\n",
    "        \n",
    "        # Define the hidden expansion size (must match your init_weights)\n",
    "        z_tube_dim = int(8/3 * self.emb)\n",
    "        \n",
    "        for l in reversed(range(self.layers)):\n",
    "            h_curr, h_prev = cache[f'h_{l+1}'], cache[f'h_{l}']\n",
    "            h_prev_flat = h_prev.reshape(-1, self.emb)\n",
    "            dh_flat = dh.reshape(-1, self.emb)\n",
    "            expert_idx = cache[f'expert_idx_l{l}']\n",
    "            \n",
    "            # Backprop through Tanh\n",
    "            dtanh = dh_flat * (1 - h_curr.reshape(-1, self.emb)**2)\n",
    "            \n",
    "            # 1. Backprop through W_down (The SwiGLU \"Muscle\")\n",
    "            # Shapes: z_flat and gate_flat are (B*T, z_tube_dim)\n",
    "            z_flat = cache[f'z_l{l}'].reshape(-1, z_tube_dim)\n",
    "            gate_flat = cache[f'gate_l{l}'].reshape(-1, z_tube_dim)\n",
    "            \n",
    "            # Gradient for the down-projection matrix\n",
    "            grads[f'W_down_l{l}'] = (z_flat * gate_flat).T @ dtanh\n",
    "            \n",
    "            # Error signal flowing back into the two branches\n",
    "            delementwise = dtanh @ self.W[f'W_down_l{l}'].T\n",
    "            \n",
    "            # 2. Split into Gate and Up-projection (LNN branch)\n",
    "            # dz_filtered is the error destined for the LNN\n",
    "            dz_filtered = delementwise * gate_flat\n",
    "            d_gate = delementwise * z_flat\n",
    "            \n",
    "            # W_up grad (Up-projection of the Liquid State)\n",
    "            h_liq_flat = cache[f'h_liq_l{l}'].reshape(-1, self.emb)\n",
    "            grads[f'W_up_l{l}'] = h_liq_flat.T @ dz_filtered\n",
    "            \n",
    "            # W_gate grad (Swish derivative on the Gate branch)\n",
    "            x_g = h_prev_flat @ self.W[f'W_gate_l{l}']\n",
    "            sig = 1 / (1 + np.exp(-x_g))\n",
    "            dx_g = d_gate * (sig * (1 + x_g * (1 - sig)))\n",
    "            grads[f'W_gate_l{l}'] = h_prev_flat.T @ dx_g\n",
    "            \n",
    "            # 3. Router Gradient (Kept as requested)\n",
    "            d_route = dtanh @ self.W['router']\n",
    "            grads['router'] += h_prev_flat.T @ d_route * 1e-4\n",
    "            \n",
    "            # 4. Error signal for LNN (Flows back through W_up)\n",
    "            dh_liquid = dz_filtered @ self.W[f'W_up_l{l}'].T\n",
    "            \n",
    "            for e in range(self.n_experts):\n",
    "                mask = (expert_idx == e)\n",
    "                \n",
    "                # Retrieve the saved flat context (512, 384)\n",
    "                ctx = cache[f'context_l{l}'] \n",
    "                \n",
    "                # Slice both to match the tokens owned by this expert\n",
    "                d_target = dh_liquid[mask]\n",
    "                ctx_selected = ctx[mask]\n",
    "                h_prev_selected = h_prev_flat[mask]\n",
    "\n",
    "                # Now the math aligns: (384, N) @ (N, 384) = (384, 384)\n",
    "                grads[f'Wo_e{e}_l{l}'] += ctx_selected.T @ d_target\n",
    "                grads[f'Wq_e{e}_l{l}'] += h_prev_selected.T @ d_target\n",
    "        \n",
    "            # MLA Up-projection Grad (Shared KV)\n",
    "            dh_reshaped = dh_liquid.reshape(B, T, self.emb)\n",
    "            dh_up = np.concatenate([dh_reshaped, dh_reshaped], axis=-1) \n",
    "            grads[f'W_kv_up_l{l}'] = (h_prev.transpose(0, 2, 1) @ dh_up).sum(0)\n",
    "\n",
    "            # Update dh for the layer below (Residual connection)\n",
    "            # We add the liquid error to the previous dh signal\n",
    "            dh = dh_reshaped + (dh_reshaped @ self.W[f'W_kv_up_l{l}'][:, :self.emb].T)\n",
    "\n",
    "            grads['W_recur_gate'] += (h_final.transpose(0, 2, 1).reshape(self.emb, -1) @ dh_flat).sum(axis=1, keepdims=True) * 0.01\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update(self, grads, lr_max, warmup, wd):\n",
    "        self.t += 1\n",
    "        # Re-introducing your old Learning Rate schedule\n",
    "        lr = lr_max * min(1.0, self.t / warmup)\n",
    "        for k in self.W:\n",
    "            g = np.clip(grads[k], -1.0, 1.0) # Grad clipping from old code\n",
    "            \n",
    "            # Adam buffers\n",
    "            self.m[k] = 0.9 * self.m[k] + 0.1 * g\n",
    "            self.v[k] = 0.999 * self.v[k] + 0.001 * (g**2)\n",
    "            \n",
    "            m_hat = self.m[k] / (1 - 0.9**self.t)\n",
    "            v_hat = self.v[k] / (1 - 0.999**self.t)\n",
    "            \n",
    "            # Update with Weight Decay\n",
    "            update_val = lr * (m_hat / (np.sqrt(v_hat) + 1e-8) + wd * self.W[k])\n",
    "            self.W[k] -= update_val\n",
    "\n",
    "    def generate(self, prompt, tok, length=50, tmp=0.7, k=40, p=0.9):\n",
    "        ids = tok.encode(prompt.lower())\n",
    "        for _ in range(length):\n",
    "            context = np.array(ids[-self.b_size:]).reshape(1, -1)\n",
    "            logits = self.forward(context)[0, -1, :].astype(np.float32)\n",
    "            \n",
    "            # 1. Repetition Penalty (The \"Anti-Stutter\" Filter)\n",
    "            for prev_id in set(ids[-15:]): \n",
    "                logits[prev_id] -= 2.0 \n",
    "\n",
    "            # 2. Temperature scaling\n",
    "            logits /= (tmp + 1e-10)\n",
    "            \n",
    "            # Top-K / Top-P logic restored from old code\n",
    "            if k > 0:\n",
    "                threshold = np.partition(logits, -k)[-k]\n",
    "                logits[logits < threshold] = -1e4\n",
    "            \n",
    "            probs = np.exp(logits - np.max(logits))\n",
    "            probs /= (probs.sum() + 1e-10)\n",
    "            \n",
    "            if p < 1.0:\n",
    "                sorted_indices = np.argsort(probs)[::-1]\n",
    "                sorted_probs = probs[sorted_indices]\n",
    "                cumulative_probs = np.cumsum(sorted_probs)\n",
    "                to_remove = cumulative_probs > p\n",
    "                to_remove[1:] = to_remove[:-1].copy()\n",
    "                to_remove[0] = False\n",
    "                probs[sorted_indices[to_remove]] = 0\n",
    "                probs /= (probs.sum() + 1e-10)\n",
    "                \n",
    "            next_id = np.random.choice(len(probs), p=probs)\n",
    "            ids.append(next_id)\n",
    "            print(tok.decode([next_id]), end=\"\", flush=True)\n",
    "\n",
    "# --- 3. EXECUTION ---\n",
    "path = 'C:\\\\Users\\\\user\\\\Downloads\\\\BNCCorpus.txt'\n",
    "tokenizer = BPETokenizer(1024)\n",
    "try:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()[:100000]\n",
    "    tokenizer.train(text)\n",
    "    data = np.array(tokenizer.encode(text))\n",
    "except:\n",
    "    print(\"Data not found.\"); sys.exit()\n",
    "\n",
    "# Init model with Old-style config\n",
    "gpt = SovereignGPT(v_size=1024, emb=384, layers=3, b_size=128, n_experts=1, cycles=3)\n",
    "\n",
    "print(\"Training Sovereign System (Old Hierarchy + New Logic)...\")\n",
    "for i in range(301):\n",
    "    xb, yb = gpt.get_batch(data, batch_size=4)\n",
    "    logits, cache = gpt.forward(xb, return_cache=True)\n",
    "    grads = gpt.backward(yb, logits, cache)\n",
    "\n",
    "    # CLIP GRADIENS: Prevent values from exploding\n",
    "    for k in grads:\n",
    "    # Calculate the global norm of the gradient\n",
    "        gnorm = np.linalg.norm(grads[k])\n",
    "        if gnorm > 1.0:\n",
    "            grads[k] = grads[k] * (1.0 / gnorm)\n",
    "    \n",
    "    # Adaptive LR\n",
    "    lr = 0.001 * (0.5 * (1 + np.cos(np.pi * i / 1000)))\n",
    "    gpt.update(grads, lr, warmup=100, wd=0.1)\n",
    "    \n",
    "    if i % 25 == 0:\n",
    "        loss = -np.mean(np.log(gpt.softmax(logits.reshape(-1, 1024))[np.arange(len(yb.flat)), yb.flat] + 1e-9))\n",
    "        print(f\"\\nStep {i} | Loss: {loss:.4f}\")\n",
    "        gpt.generate(\"The \", tokenizer, length=7)\n",
    "        print(\"\\n\" + \"-\"*30)\n",
    "\n",
    "print(\"\\n--- Sovereign Chat ---\")\n",
    "while True:\n",
    "    u = input(\"\\nSovereign> \").strip()\n",
    "    if u.lower() in ['q', 'exit']: break\n",
    "    gpt.generate(u, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "\n",
    "# --- TOKENIZER (BPE) ---\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=512):\n",
    "        self.vocab_size, self.merges, self.vocab = vocab_size, {}, {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "    def train(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        for i in range(self.vocab_size - 256):\n",
    "\n",
    "            stats = {}\n",
    "            for pair in zip(tokens, tokens[1:]): stats[pair] = stats.get(pair, 0) + 1\n",
    "            if not stats: break\n",
    "\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            self.merges[pair], self.vocab[idx] = idx, self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "            new_tokens, j = [], 0\n",
    "\n",
    "            while j < len(tokens):\n",
    "                if j < len(tokens)-1 and (tokens[j], tokens[j+1]) == pair:\n",
    "                    new_tokens.append(idx); j += 2\n",
    "                else: new_tokens.append(tokens[j]); j += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) >= 2:\n",
    "\n",
    "            stats = {p: self.merges[p] for p in zip(tokens, tokens[1:]) if p in self.merges}\n",
    "            if not stats: break\n",
    "\n",
    "            pair = min(stats.keys(), key=lambda p: self.merges[p])\n",
    "            idx, new_tokens, j = self.merges[pair], [], 0\n",
    "\n",
    "            while j < len(tokens):\n",
    "                if j < len(tokens)-1 and (tokens[j], tokens[j+1]) == pair:\n",
    "                    new_tokens.append(idx); j += 2\n",
    "                else: new_tokens.append(tokens[j]); j += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return b\"\".join(self.vocab[idx] for idx in ids).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "# --- MODEL ---\n",
    "class GPT:\n",
    "    def __init__(self, v_size, emb, h, groups, layers, b_size):\n",
    "        self.v_size, self.emb, self.h, self.layers, self.b_size = v_size, emb, h, layers, b_size\n",
    "        self.groups, self.head_dim = groups, emb // h\n",
    "        self.heads_per_group = h // groups \n",
    "\n",
    "        self.mask = np.triu(np.ones((b_size, b_size)), 1).astype(bool)\n",
    "        inv = 1.0 / (10000 ** (np.arange(0, self.head_dim, 2) / self.head_dim))\n",
    "        freqs = np.outer(np.arange(b_size), inv)\n",
    "        self.cos_pre, self.sin_pre = np.cos(freqs).astype(np.float16), np.sin(freqs).astype(np.float16)\n",
    "\n",
    "        self.W, self.kv_cache = {}, [None] * layers\n",
    "        self.init_w()\n",
    "        self.t = 0\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.W.items()}\n",
    "        self.v = {k: np.zeros_like(v) for k, v in self.W.items()}\n",
    "\n",
    "    def init_w(self):\n",
    "        self.W['E'] = (np.random.randn(self.v_size, self.emb) * 0.02).astype(np.float16)\n",
    "        for l in range(self.layers):\n",
    "            self.W[f'g1{l}'] = np.ones((1,1,self.emb), dtype=np.float16)\n",
    "            self.W[f'g2{l}'] = np.ones((1,1,self.emb), dtype=np.float16)\n",
    "\n",
    "            std = np.sqrt(2.0 / (2 * self.emb))\n",
    "            self.W[f'Wq{l}'] = (np.random.randn(self.emb, self.emb) * std).astype(np.float16)\n",
    "            self.W[f'Wk{l}'] = (np.random.randn(self.emb, self.groups * self.head_dim) * std).astype(np.float16)\n",
    "            self.W[f'Wv{l}'] = (np.random.randn(self.emb, self.groups * self.head_dim) * std).astype(np.float16)\n",
    "            self.W[f'Wo{l}'] = (np.random.randn(self.emb, self.emb) * std).astype(np.float16)\n",
    "            \n",
    "            hidden = int(8/3 * self.emb)\n",
    "            self.W[f'Wf1{l}'] = (np.random.randn(self.emb, hidden) * np.sqrt(2/self.emb)).astype(np.float16)\n",
    "            self.W[f'Wf2{l}'] = (np.random.randn(hidden, self.emb) * np.sqrt(2/hidden)).astype(np.float16)\n",
    "            self.W[f'Wf3{l}'] = (np.random.randn(self.emb, hidden) * np.sqrt(2/self.emb)).astype(np.float16)\n",
    "\n",
    "    def rms_norm(self, x, g, out=None):\n",
    "        if out is None: out = np.empty_like(x)\n",
    "        msq = np.mean(x.astype(np.float32)**2, axis=-1, keepdims=True)\n",
    "        out[:] = (g * (x.astype(np.float32) * (msq + 1e-6)**-0.5)).astype(np.float16)\n",
    "        return out.astype(np.float16)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x_32 = x.astype(np.float32)\n",
    "        e_x = np.exp(x_32 - np.max(x_32, axis=-1, keepdims=True))\n",
    "        out = e_x / (e_x.sum(axis=-1, keepdims=True) + 1e-10)\n",
    "        return out.astype(np.float16)\n",
    "\n",
    "    def apply_rope(self, x, pos_idx, rev=False):\n",
    "        c, s = self.cos_pre[pos_idx], self.sin_pre[pos_idx]\n",
    "        if rev: s = -s\n",
    "        x1, x2 = x[..., 0::2], x[..., 1::2]\n",
    "        out = np.empty_like(x)\n",
    "        out[..., 0::2] = x1 * c - x2 * s\n",
    "        out[..., 1::2] = x1 * s + x2 * c\n",
    "        return out\n",
    "\n",
    "    def get_batch(self, data, batch_size):\n",
    "        ix = np.random.randint(0, len(data) - self.b_size, batch_size)\n",
    "        x = np.empty((batch_size, self.b_size), dtype=np.int32)\n",
    "        y = np.empty((batch_size, self.b_size), dtype=np.int32)\n",
    "        for i, idx in enumerate(ix):\n",
    "            x[i] = data[idx : idx + self.b_size]\n",
    "            y[i] = data[idx + 1 : idx + self.b_size + 1]\n",
    "        return x, y\n",
    "\n",
    "    def forward(self, idx, start_pos=0, use_cache=False):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Transform discrete Token IDs into continuous 256-dim vectors\n",
    "        # Shape: (B, T) -> (B, T, 256)\n",
    "        x = self.W['E'][idx].astype(np.float16)\n",
    "\n",
    "        pos_idx = np.arange(start_pos, start_pos + T)\n",
    "        cache = {'idx': idx, 'x_0': x.copy()} \n",
    "        \n",
    "        for l in range(self.layers):\n",
    "            ln1 = self.rms_norm(x, self.W[f'g1{l}'])\n",
    "\n",
    "            # Project normalized input into Query, Key, and Value spaces\n",
    "            # Q: (B, T, 256) @ (256, 256) -> (B, 8, T, 32) [Split into 8 heads]\n",
    "            # K/V: (B, T, 256) @ (256, 64) -> (B, 2, T, 32) [Split into 2 groups\n",
    "            q = (ln1 @ self.W[f'Wq{l}']).reshape(B, T, self.h, self.head_dim).transpose(0, 2, 1, 3).astype(np.float16)\n",
    "            k = (ln1 @ self.W[f'Wk{l}']).reshape(B, T, self.groups, self.head_dim).transpose(0, 2, 1, 3).astype(np.float16)\n",
    "            v = (ln1 @ self.W[f'Wv{l}']).reshape(B, T, self.groups, self.head_dim).transpose(0, 2, 1, 3).astype(np.float16)\n",
    "\n",
    "            q, k = self.apply_rope(q, pos_idx), self.apply_rope(k, pos_idx)\n",
    "            \n",
    "            if use_cache:\n",
    "                if self.kv_cache[l] is None or start_pos == 0: self.kv_cache[l] = (k, v)\n",
    "                else:\n",
    "                    k = np.concatenate([self.kv_cache[l][0], k], axis=2)\n",
    "                    v = np.concatenate([self.kv_cache[l][1], v], axis=2)\n",
    "                    self.kv_cache[l] = (k, v)\n",
    "            \n",
    "            # Expand Keys to match the number of Query heads (GQA logic)\n",
    "            # Shape: (B, 2, T, 32) -> (B, 8, T, 32)\n",
    "            k_rep = np.repeat(k, self.heads_per_group, axis=1)\n",
    "            v_rep = np.repeat(v, self.heads_per_group, axis=1)\n",
    "            \n",
    "            # Compute alignment scores: \"How much does word T look at word T-n?\"\n",
    "            # (B, 8, T, 32) @ (B, 8, 32, T) -> (B, 8, T, T)\n",
    "            sc = (q @ k_rep.transpose(0, 1, 3, 2)) / np.sqrt(self.head_dim)\n",
    "            if not use_cache: sc[:, :, self.mask[:T, :T]] = -65500\n",
    "            \n",
    "            # Convert scores to probabilities (Softmax)\n",
    "            # Result: (B, 8, T, T) summing to 1.0 across the last dimension\n",
    "            sc_32 = sc.astype(np.float32)\n",
    "            e_x = np.exp(sc_32 - np.max(sc_32, axis=-1, keepdims=True))\n",
    "            attn = (e_x / (e_x.sum(axis=-1, keepdims=True) + 1e-10)).astype(np.float16)\n",
    "            \n",
    "            # Use attention weights to mix the \"Content\" (Values)\n",
    "            # Input: (B, h, T, T) @ (B, h, T, head_dim) -> Output: (B, h, T, head_dim)\n",
    "            out_att = (attn @ v_rep).transpose(0, 2, 1, 3).reshape(B, T, self.emb)\n",
    "\n",
    "            # Project mixed content back to the residual highway and add it\n",
    "            # (B, T, self.emb) + (B, T, self.emb) -> (B, T, self.emb)\n",
    "            x += out_att @ self.W[f'Wo{l}']\n",
    "            \n",
    "            # Normalize the highway (Residual Stream)\n",
    "            # Input: (B, T, 256) -> Output: (B, T, 256)\n",
    "            ln2 = self.rms_norm(x, self.W[f'g2{l}'])\n",
    "\n",
    "            # Project to Hidden Space (Expansion)\n",
    "            # (B, T, 256) @ (256, 682) -> (B, T, 682)\n",
    "            gate = ln2 @ self.W[f'Wf1{l}']\n",
    "            up = ln2 @ self.W[f'Wf3{l}']\n",
    "\n",
    "            # Apply Activation (Swish)\n",
    "            # Shape remains (B, T, 682)\n",
    "            gate_32 = gate.astype(np.float32)\n",
    "            act_gate = (gate_32 / (1.0 + np.exp(-gate_32))).astype(np.float16)\n",
    "\n",
    "            # Element-wise multiply and Shrink back to Embedding size\n",
    "            # (B, T, 682) * (B, T, 682) -> (B, T, 682)\n",
    "            # (B, T, 682) @ (682, 256) -> (B, T, 256)\n",
    "            x += (act_gate * up) @ self.W[f'Wf2{l}']\n",
    "            cache[f'l{l}'] = (ln1, q, k, v, attn, out_att, ln2, gate, up, act_gate)\n",
    "\n",
    "        cache['x_final'] = x\n",
    "        return x @ self.W['E'].T, cache\n",
    "\n",
    "    def backward(self, yb, logits, cache):\n",
    "        B, T = yb.shape\n",
    "        grads = {k: np.zeros_like(v, dtype=np.float32) for k, v in self.W.items()}\n",
    "        pp_sum = np.exp(logits.astype(np.float32) - np.max(logits.astype(np.float32), axis=-1, keepdims=True))\n",
    "        probs = pp_sum / (pp_sum.sum(axis=-1, keepdims=True) + 1e-10)\n",
    "        probs[np.arange(B)[:, None], np.arange(T), yb] -= 1\n",
    "\n",
    "        # Calculate Cross-Entropy Loss gradient (Probs - Ground Truth)\n",
    "        # Shape: (B, T, Vocab)\n",
    "        dl = (probs / (B * T)).astype(np.float16)\n",
    "        \n",
    "        # Calculate error for output projection\n",
    "        # (B, T, vocab) @ (B, T, 256) -> (Vocab, 256)\n",
    "        grads['E'] += (dl.transpose(0, 2, 1) @ cache['x_final']).sum(0)\n",
    "\n",
    "        # Backprop error into the residual stream 'dx'\n",
    "        # (B, T, Vocab) @ (Vocab, Emb) -> (B, T, Emb)\n",
    "        dx = (dl @ self.W['E']).astype(np.float16)\n",
    "        pos_idx = np.arange(T)\n",
    "\n",
    "        for l in reversed(range(self.layers)):\n",
    "            ln1, q, k, v, attn, out_att, ln2, gate, up, act_gate = cache[f'l{l}']\n",
    "\n",
    "            # Move error through the SwiGLU bottleneck\n",
    "            # (B, T, 256) @ (256, 682) -> (B, T, 682)\n",
    "            df2 = dx @ self.W[f'Wf2{l}'].T\n",
    "\n",
    "            # Gradient for Wf2: Input (act_gate) @ Error (dx)\n",
    "            # (Hidden, B*T) @ (B*T, Emb) -> (Hidden, Emb)\n",
    "            grads[f'Wf2{l}'] += act_gate.reshape(-1, act_gate.shape[-1]).T @ dx.reshape(-1, self.emb)\n",
    "            \n",
    "            sig = 1 / (1 + np.exp(-gate.astype(np.float32)))\n",
    "            dgate32 = (df2.astype(np.float32) * up) * (sig * (1 + gate.astype(np.float32) * (1 - sig)))\n",
    "            dgate = dgate32.astype(np.float16)\n",
    "            dup = (df2 * act_gate).astype(np.float16)\n",
    "\n",
    "            # Gradient for Weight f3 (Up projection)\n",
    "            # (256, B*T) @ (B*T, 682) -> (256, 682)\n",
    "            grads[f'Wf3{l}'] += ln2.reshape(-1, self.emb).T @ dup.reshape(-1, dup.shape[-1])\n",
    "            grads[f'Wf1{l}'] += ln2.reshape(-1, self.emb).T @ dgate.reshape(-1, dgate.shape[-1])\n",
    "            \n",
    "            # Update dx with the error flowing through the MLP back to the main highway\n",
    "            # (B, T, Hidden) @ (Hidden, Emb) -> (B, T, Emb)\n",
    "            dx += (dgate @ self.W[f'Wf1{l}'].T) + (dup @ self.W[f'Wf3{l}'].T)\n",
    "            \n",
    "            grads[f'Wo{l}'] += out_att.reshape(-1, self.emb).T @ dx.reshape(-1, self.emb)\n",
    "            \n",
    "            # Error for the attention mechanism result\n",
    "            # (B, T, Emb) @ (Emb, Emb) -> (B, 8, T, 32)\n",
    "            dout = (dx @ self.W[f'Wo{l}'].T).reshape(B, T, self.h, self.head_dim).transpose(0, 2, 1, 3)\n",
    "            \n",
    "            # Error for V: Sum heads back into groups\n",
    "            # (B, 8, T, T).T @ (B, 8, T, 32) -> (B, 2, T, 32)\n",
    "            dv = (attn.transpose(0, 1, 3, 2) @ dout).reshape(B, self.groups, self.heads_per_group, T, self.head_dim).sum(2)\n",
    "            \n",
    "            # Error for Attention Scores (Softmax derivative)\n",
    "            da = (dout @ np.repeat(v, self.heads_per_group, axis=1).transpose(0, 1, 3, 2))\n",
    "            da_f32 = da.astype(np.float32)\n",
    "            attn_f32 = attn.astype(np.float32)\n",
    "            ds = attn_f32 * (da_f32 - (attn_f32 * da_f32).sum(-1, keepdims=True))\n",
    "            ds = ds.astype(np.float16)\n",
    "\n",
    "            # Error for Q and K (Untwist RoPE first)\n",
    "            dq = self.apply_rope(ds @ np.repeat(k, self.heads_per_group, axis=1), pos_idx, rev=True) / np.sqrt(self.head_dim)\n",
    "            dk = self.apply_rope(ds.transpose(0, 1, 3, 2) @ q, pos_idx, rev=True).reshape(B, self.groups, self.heads_per_group, T, self.head_dim).sum(2) / np.sqrt(self.head_dim)\n",
    "            \n",
    "            # Backprop to Q, K, V Projection weights\n",
    "            # (Emb, B*T) @ (B*T, Head_Dim) -> (Emb, Emb)\n",
    "            grads[f'Wq{l}'] += ln1.reshape(-1, self.emb).T @ dq.transpose(0, 2, 1, 3).reshape(-1, self.emb)\n",
    "            grads[f'Wk{l}'] += ln1.reshape(-1, self.emb).T @ dk.transpose(0, 2, 1, 3).reshape(-1, self.groups*self.head_dim)\n",
    "            grads[f'Wv{l}'] += ln1.reshape(-1, self.emb).T @ dv.transpose(0, 2, 1, 3).reshape(-1, self.groups*self.head_dim)\n",
    "            \n",
    "            dx += (dq.transpose(0, 2, 1, 3).reshape(-1, self.emb) @ self.W[f'Wq{l}'].T).reshape(B, T, self.emb)\n",
    "        \n",
    "        np.add.at(grads['E'], cache['idx'], dx)\n",
    "        return grads\n",
    "\n",
    "    def update(self, grads, step, lr_max, warmup, wd):\n",
    "        self.t += 1\n",
    "        lr = lr_max * min(1.0, step/warmup) * max(0.1, 1.0 - (step-warmup)/4000)\n",
    "        for k in self.W:\n",
    "            g = np.clip(grads[k], -1.0, 1.0)\n",
    "\n",
    "            self.m[k] = (0.9 * self.m[k].astype(np.float32) + 0.1 * g)\n",
    "            self.v[k] = (0.999 * self.v[k].astype(np.float32) + 0.001 * (g**2))\n",
    "\n",
    "            mh = self.m[k] / (1 - 0.9**self.t)\n",
    "            vh = self.v[k] / (1 - 0.999**self.t)\n",
    "            \n",
    "            update_val = lr * (mh / (np.sqrt(vh) + 1e-8) + wd * self.W[k].astype(np.float32))\n",
    "            self.W[k] = (self.W[k].astype(np.float32) - update_val).astype(np.float16)\n",
    "        \n",
    "        if self.t % 25 == 0:\n",
    "                for k, g in grads.items():\n",
    "                    print(f\"{k} | Mean: {np.mean(g):.6f} | Std: {np.std(g):.6f}\")\n",
    "\n",
    "    def generate(self, prompt, tok, length=30, tmp=0.7, k=40, p=0.9):\n",
    "        ids = tok.encode(prompt)\n",
    "        self.kv_cache = [None] * self.layers\n",
    "        curr_ids = np.array(ids).reshape(1, -1)\n",
    "\n",
    "        for _ in range(length):\n",
    "            logits, _ = self.forward(curr_ids, start_pos=len(ids)-curr_ids.shape[1], use_cache=True)\n",
    "            logits = logits[0, -1, :].astype(np.float32) / (tmp + 1e-10)\n",
    "\n",
    "            if k > 0: logits[logits < np.partition(logits, -k)[-k]] = -1e4\n",
    "            probs = np.exp(logits - np.max(logits))\n",
    "            probs /= (probs.sum() + 1e-10)\n",
    "\n",
    "            if p < 1.0:\n",
    "                si = np.argsort(probs)[::-1]; sp = probs[si]; cp = np.cumsum(sp)\n",
    "                ir = cp > p; ir[1:] = ir[:-1].copy(); ir[0] = False\n",
    "                probs[si[ir]] = 0; probs /= (probs.sum() + 1e-10)\n",
    "                \n",
    "            next_id = np.random.choice(len(probs), p=probs)\n",
    "            ids.append(next_id); curr_ids = np.array([[next_id]])\n",
    "            print(tok.decode([next_id]), end=\"\", flush=True)\n",
    "        return ids\n",
    "\n",
    "# --- CONFIG ---\n",
    "path = \"C:\\\\Users\\\\user\\\\Downloads\\\\BNCCorpus.txt\" \n",
    "weight_path = \"C:\\\\Users\\\\user\\\\Downloads\\\\model_weights.npz\"\n",
    "token_path = \"C:\\\\Users\\\\user\\\\Downloads\\\\tokenized_data.npy\"\n",
    "vocab_size, block_size, embed, groups, heads, n_layers = 384, 128, 64, 2, 4, 3\n",
    "lr_max, batch_size, warmup, accum_steps, wd = 0.0001, 4, 200, 1, 0.1\n",
    "\n",
    "# --- EXECUTION ---\n",
    "try:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f: text = f.read().lower()[:150000]\n",
    "except: print('No data...'); sys.exit()\n",
    "\n",
    "tokenizer = BPETokenizer(vocab_size)\n",
    "if os.path.exists(token_path):\n",
    "    token_data = np.load(token_path)\n",
    "    print(\"Loaded pre-tokenized data.\")\n",
    "else:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f: \n",
    "        text = f.read().lower()[:150000]\n",
    "    print(\"Tokenizing text (this may take a few minutes once)...\")\n",
    "    token_data = np.array(tokenizer.encode(text), dtype=np.uint16)\n",
    "    np.save(token_path, token_data)\n",
    "    print(\"Tokens saved.\")\n",
    "\n",
    "n = int(0.9 * len(token_data))\n",
    "train_data, val_data = token_data[:n], token_data[n:]\n",
    "\n",
    "gpt = GPT(vocab_size, embed, heads, groups, n_layers, block_size)\n",
    "if os.path.exists(weight_path):\n",
    "    cp = np.load(weight_path)\n",
    "\n",
    "    for k in gpt.W:\n",
    "        if k in cp: gpt.W[k] = cp[k]\n",
    "        if f\"m_{k}\" in cp: gpt.m[k], gpt.v[k] = cp[f\"m_{k}\"], cp[f\"v_{k}\"]\n",
    "    print(\"Existing weights loaded.\")\n",
    "    mode = input(\"\\n[T]rain further or [C]hat only? \").lower()\n",
    "\n",
    "else:\n",
    "    print(\"No weights found. Starting fresh training...\")\n",
    "    mode = 't'\n",
    "\n",
    "if mode == 't':\n",
    "    token_data = np.array(tokenizer.encode(text), dtype=np.uint16)\n",
    "    n = int(0.9 * len(token_data))\n",
    "    train_data, val_data = token_data[:n], token_data[n:]\n",
    "    acc_grads = {k: np.zeros_like(v, dtype=np.float32) for k, v in gpt.W.items()}\n",
    "    \n",
    "    for i in range(401):\n",
    "        xb, yb = gpt.get_batch(train_data, batch_size)\n",
    "        logits, cache = gpt.forward(xb)\n",
    "        grads = gpt.backward(yb, logits, cache)\n",
    "        for k in grads: acc_grads[k] += np.clip(grads[k], -1.0, 1.0)\n",
    "        del cache  \n",
    "        del grads\n",
    "\n",
    "        if i % 1 == 0: \n",
    "            print(f\"Iteration {i} started...\", end='\\r')\n",
    "        \n",
    "        if (i + 1) % accum_steps == 0:\n",
    "            for k in acc_grads: acc_grads[k] /= accum_steps\n",
    "            gpt.update(acc_grads, i // accum_steps, lr_max, warmup, wd)\n",
    "            for k in acc_grads: acc_grads[k].fill(0)\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            xv, yv = gpt.get_batch(val_data, batch_size)\n",
    "            v_logits, _ = gpt.forward(xv)\n",
    "            v_probs = gpt.softmax(v_logits)\n",
    "            v_loss = -np.mean(np.log(v_probs[np.arange(batch_size)[:,None], np.arange(block_size), yv] + 1e-10))\n",
    "            \n",
    "            save_dict = {**gpt.W, **{f\"m_{k}\": gpt.m[k] for k in gpt.W}, **{f\"v_{k}\": gpt.v[k] for k in gpt.W}}\n",
    "            np.savez(weight_path, **save_dict)\n",
    "            print(f\"Step {i:5d} | Loss: {v_loss:.4f} \", flush=True)\n",
    "\n",
    "print(\"\\n--- Chatting ---\")\n",
    "while True:\n",
    "    u = input(\"\\n> \").strip()\n",
    "    if u in ['q', 'exit']: break\n",
    "    gpt.generate(u, tokenizer,  length=60, tmp=0.8)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8475967",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_games = [\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.9, 0.9, 0.9], \n",
    "    [0.8, 0.2, 0.5],\n",
    "]\n",
    "results = [0, 0, 1]\n",
    "\n",
    "def knn_predict(new_data, dataset, targets, k=3):\n",
    "    distances = []\n",
    "    for i in range(len(dataset)):\n",
    "        dist = sum((new_data[j] - dataset[i][j])**2 for j in range(len(new_data)))**0.5\n",
    "        distances.append((dist, targets[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    nearest = [d[1] for d in distances[:k]]\n",
    "    return \"Хит\" if max(set(nearest), key=nearest.count) == 0 else \"Провал\"\n",
    "\n",
    "test_game = [0.15, 0.85, 0.12]\n",
    "print(f\"Вердикт KNN: {knn_predict(test_game, old_games, results, k=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afff137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_predict(game):\n",
    "    if game[1] > 0.8:\n",
    "        if game[0] < 0.3: return \"Инди-хит\"\n",
    "        else: return \"Блокбастер\"\n",
    "    else: return \"Провал\"\n",
    "\n",
    "new_game = [0.1, 0.9, 0.5]\n",
    "print(f\"Вердикт дерева: {tree_predict(new_game)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "dataset = [[0.1, 0.8, 0.1], [0.9, 0.9, 0.9], [0.8, 0.2, 0.5]]\n",
    "targets = [1, 1, -1] \n",
    "weights = [random.uniform(-0.1, 0.1) for _ in range(3)]\n",
    "bias = 0.0\n",
    "lr = 0.01 * 2\n",
    "C = 1.0\n",
    "epochs = 1000\n",
    "l1_param = 0.005\n",
    "l2_param = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, x in enumerate(dataset):\n",
    "        condition = targets[i] * (sum(x[j] * weights[j] for j in range(3)) + bias)\n",
    "        if condition >= 1:\n",
    "            for j in range(3):\n",
    "                weights[j] -= lr * (l2_param * weights[j] + l1_param * (1 if weights[j] > 0 else -1))\n",
    "        else:\n",
    "            for j in range(3):\n",
    "                weights[j] -= lr * (l2_param * weights[j] - C * x[j] * targets[i])\n",
    "            bias += lr * C * targets[i]\n",
    "\n",
    "test_game = [0.15, 0.85, 0.12]\n",
    "result = sum(test_game[j] * weights[j] for j in range(3)) + bias\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "prob_hit = sigmoid(result)\n",
    "print(f\"SVM вердикт: {'Хит' if result > 0 else 'Провал'}\")\n",
    "print(f\"Счет (Score): {result:.2f}\")\n",
    "print(f\"Уверенность (Вероятность Хима): {prob_hit:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
