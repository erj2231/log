{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. DATA PREP ---\n",
    "try:\n",
    "    with open(\"C:\\\\Users\\\\User\\\\Downloads\\\\BNCCorpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().lower()[:50000]\n",
    "        text = text.replace(\". \", \".\\nme: \").replace(\"? \", \"?\\nai: \")\n",
    "except:\n",
    "    text = \"hello world. this is a test string to ensure the model works. \" * 500\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab = len(chars)\n",
    "char_to_id = {ch: i for i, ch in enumerate(chars)}\n",
    "id_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "data = np.array([char_to_id[c] for c in text])\n",
    "\n",
    "# --- 2. CONFIG ---\n",
    "block_size = 64  # T\n",
    "embed = 128       # C\n",
    "heads = 4         # H\n",
    "head_dim = 32     # D (embed // heads)\n",
    "hid = 256         # Feed-forward dim\n",
    "lr = 0.001\n",
    "batch_size = 8    # B\n",
    "\n",
    "# --- 3. PARAMETERS ---\n",
    "def init(r, c): return np.random.randn(r, c) * np.sqrt(2.0 / (r + c))\n",
    "\n",
    "W = {\n",
    "    'E': init(vocab, embed), 'Pos': init(block_size, embed),\n",
    "    'Wq': init(embed, embed), 'Wk': init(embed, embed), 'Wv': init(embed, embed),\n",
    "    'Wproj': init(embed, embed), 'W1': init(embed, hid), 'Wout': init(hid, vocab),\n",
    "    'LNg': np.ones((1, embed)), 'LNb': np.zeros((1, embed))\n",
    "}\n",
    "B_params = {'B1': np.zeros((1, hid)), 'Bout': np.zeros((1, vocab))}\n",
    "\n",
    "m = {k: np.zeros_like(v) for k, v in {**W, **B_params}.items()}\n",
    "v = {k: np.zeros_like(v) for k, v in {**W, **B_params}.items()}\n",
    "t_adam = 0\n",
    "\n",
    "def softmax(x):\n",
    "    e = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e / np.sum(e, axis=-1, keepdims=True)\n",
    "\n",
    "# --- 4. ENGINE ---\n",
    "def train_step(idx, targets):\n",
    "    global t_adam\n",
    "    B_batch, T = idx.shape # (B, T)\n",
    "    \n",
    "    # FORWARD\n",
    "    e_x = W['E'][idx]      # (B, T, C)\n",
    "    p_x = W['Pos'][:T]     # (T, C)\n",
    "    x_raw = e_x + p_x      # (B, T, C) via broadcasting\n",
    "    \n",
    "    mean = np.mean(x_raw, axis=-1, keepdims=True) # (B, T, 1)\n",
    "    std = np.std(x_raw, axis=-1, keepdims=True) + 1e-5 # (B, T, 1)\n",
    "    x = W['LNg'] * (x_raw - mean) / std + W['LNb']     # (B, T, C)\n",
    "    \n",
    "    Q_full = x @ W['Wq'] # (B, T, C)\n",
    "    K_full = x @ W['Wk'] # (B, T, C)\n",
    "    V_full = x @ W['Wv'] # (B, T, C)\n",
    "    \n",
    "    Q = Q_full.reshape(B_batch, T, heads, head_dim).transpose(0, 2, 1, 3) # (B, H, T, D)\n",
    "    K = K_full.reshape(B_batch, T, heads, head_dim).transpose(0, 2, 1, 3) # (B, H, T, D)\n",
    "    V = V_full.reshape(B_batch, T, heads, head_dim).transpose(0, 2, 1, 3) # (B, H, T, D)\n",
    "\n",
    "    scores = (Q @ K.transpose(0, 1, 3, 2)) / np.sqrt(head_dim) # (B, H, T, T)\n",
    "    mask = np.triu(np.ones((T, T)), k=1).astype(bool)          # (T, T)\n",
    "    scores[:, :, mask] = -1e9\n",
    "    attn = softmax(scores) # (B, H, T, T)\n",
    "    \n",
    "    context_heads = attn @ V # (B, H, T, D)\n",
    "    context_merged = context_heads.transpose(0, 2, 1, 3).reshape(B_batch, T, embed) # (B, T, C)\n",
    "    context_proj = context_merged @ W['Wproj'] # (B, T, C)\n",
    "    \n",
    "    hid_logits = np.maximum(0, context_proj @ W['W1'] + B_params['B1']) # (B, T, hid)\n",
    "    logits = hid_logits @ W['Wout'] + B_params['Bout']                 # (B, T, vocab)\n",
    "    probs = softmax(logits) # (B, T, vocab)\n",
    "\n",
    "    # BACKWARD\n",
    "    t_adam += 1\n",
    "    gradient = probs.copy() # (B, T, vocab)\n",
    "    gradient[np.arange(B_batch)[:, None], np.arange(T), targets] -= 1\n",
    "    gradient /= (B_batch * T) \n",
    "\n",
    "    hid_logits_flat = hid_logits.reshape(-1, hid) # (B*T, hid)\n",
    "    gradient_flat = gradient.reshape(-1, vocab)   # (B*T, vocab)\n",
    "    \n",
    "    dWout = hid_logits_flat.T @ gradient_flat # (hid, vocab)\n",
    "    dBout = np.sum(gradient_flat, axis=0, keepdims=True) # (1, vocab)\n",
    "    \n",
    "    dh = (gradient_flat @ W['Wout'].T) * (hid_logits_flat > 0) # (B*T, hid)\n",
    "    dW1 = context_proj.reshape(-1, embed).T @ dh # (C, hid)\n",
    "    dB1 = np.sum(dh, axis=0, keepdims=True)       # (1, hid)\n",
    "    \n",
    "    d_context_proj = (dh @ W['W1'].T).reshape(B_batch, T, embed) # (B, T, C)\n",
    "    dWproj = context_merged.reshape(-1, embed).T @ d_context_proj.reshape(-1, embed) # (C, C)\n",
    "    \n",
    "    d_context_merged = d_context_proj @ W['Wproj'].T # (B, T, C)\n",
    "    d_context_heads = d_context_merged.reshape(B_batch, T, heads, head_dim).transpose(0, 2, 1, 3) # (B, H, T, D)\n",
    "    \n",
    "    dV_heads = attn.transpose(0, 1, 3, 2) @ d_context_heads # (B, H, T, D)\n",
    "    d_attn = d_context_heads @ V.transpose(0, 1, 3, 2)     # (B, H, T, T)\n",
    "    \n",
    "    d_scores = attn * (d_attn - np.sum(d_attn * attn, axis=-1, keepdims=True)) # (B, H, T, T)\n",
    "    d_scores /= np.sqrt(head_dim)\n",
    "    \n",
    "    dQ_heads = d_scores @ K # (B, H, T, D)\n",
    "    dK_heads = d_scores.transpose(0, 1, 3, 2) @ Q # (B, H, T, D)\n",
    "    \n",
    "    dQ = dQ_heads.transpose(0, 2, 1, 3).reshape(-1, embed) # (B*T, C)\n",
    "    dK = dK_heads.transpose(0, 2, 1, 3).reshape(-1, embed) # (B*T, C)\n",
    "    dV = dV_heads.transpose(0, 2, 1, 3).reshape(-1, embed) # (B*T, C)\n",
    "    \n",
    "    x_flat = x.reshape(-1, embed) # (B*T, C)\n",
    "    dWq, dWk, dWv = x_flat.T @ dQ, x_flat.T @ dK, x_flat.T @ dV # (C, C)\n",
    "    \n",
    "    dx = (dQ @ W['Wq'].T + dK @ W['Wk'].T + dV @ W['Wv'].T) # (B*T, C)\n",
    "    dx = (dx - np.mean(dx, axis=-1, keepdims=True)) / (np.std(x_raw.reshape(-1, embed), axis=-1, keepdims=True) + 1e-5)\n",
    "    \n",
    "    dE = np.zeros_like(W['E']) # (vocab, C)\n",
    "    np.add.at(dE, idx.reshape(-1), dx)\n",
    "    dPos = dx.reshape(B_batch, T, embed).mean(axis=0) # (T, C)\n",
    "\n",
    "    # UPDATE\n",
    "    updates = {'Wout':dWout, 'Bout':dBout, 'W1':dW1, 'B1':dB1, 'Wq':dWq, 'Wk':dWk, 'Wv':dWv, 'Wproj':dWproj, 'E':dE, 'Pos':dPos}\n",
    "    for k, grad in updates.items():\n",
    "        p_ref = W if k in W else B_params\n",
    "        grad = np.clip(grad, -1, 1)\n",
    "        m[k] = 0.9 * m[k] + 0.1 * grad\n",
    "        v[k] = 0.999 * v[k] + 0.001 * (grad**2)\n",
    "        p_ref[k] -= lr * (m[k] / (1-0.9**t_adam)) / (np.sqrt(v[k] / (1-0.999**t_adam)) + 1e-8)\n",
    "\n",
    "    return -np.mean(np.log(probs[np.arange(B_batch)[:, None], np.arange(T), targets] + 1e-10))\n",
    "\n",
    "# --- 5. EXECUTION ---\n",
    "for i in range(10001):\n",
    "    ix = np.random.randint(0, len(data) - block_size, batch_size)\n",
    "    xb = np.stack([data[j:j+block_size] for j in ix])\n",
    "    yb = np.stack([data[j+1:j+block_size+1] for j in ix])\n",
    "    loss = train_step(xb, yb)\n",
    "    if i % 500 == 0: print(f\"Step {i} | Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5dcda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. GENERATION ---\n",
    "def generate(prompt, length=60, k=5, temp=0.5, penalty=1.5):\n",
    "    curr = [char_to_id.get(c, 0) for c in prompt.lower() if c in char_to_id]\n",
    "    if not curr: curr = [0]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        inp = np.array(curr[-block_size:]).reshape(1, -1)\n",
    "        T = inp.shape[1]\n",
    "        \n",
    "        x = W['E'][inp] + W['Pos'][:T]\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        std = np.std(x, axis=-1, keepdims=True) + 1e-5\n",
    "        x = W['LNg'] * (x - mean) / std + W['LNb']\n",
    "        \n",
    "        Q = (x @ W['Wq']).reshape(1, T, heads, head_dim).transpose(0, 2, 1, 3)\n",
    "        K = (x @ W['Wk']).reshape(1, T, heads, head_dim).transpose(0, 2, 1, 3)\n",
    "        V = (x @ W['Wv']).reshape(1, T, heads, head_dim).transpose(0, 2, 1, 3)\n",
    "        \n",
    "        scores = (Q @ K.transpose(0, 1, 3, 2)) / np.sqrt(head_dim)\n",
    "        attn = softmax(scores)\n",
    "        context = (attn @ V).transpose(0, 2, 1, 3).reshape(1, T, embed) @ W['Wproj']\n",
    "        \n",
    "        h = np.maximum(0, context @ W['W1'] + B['B1'])\n",
    "        logits = h[0, -1:] @ W['Wout'] + B['Bout']\n",
    "        \n",
    "        # Apply Repetition Penalty\n",
    "        for char_id in set(curr[-15:]):\n",
    "            if id_to_char[char_id] != \" \":\n",
    "                logits[0, char_id] /= 1.5 \n",
    "            else:\n",
    "                logits[0, char_id] *= 1.2 \n",
    "            \n",
    "        p = softmax(logits / temp)[0]\n",
    "        top_idx = np.argsort(p)[-k:]\n",
    "        p_top = p[top_idx] / np.sum(p[top_idx])\n",
    "        curr.append(np.random.choice(top_idx, p=p_top))\n",
    "        \n",
    "    return \"\".join([id_to_char[i] for i in curr])\n",
    "\n",
    "\n",
    "print(\"\\n--- Chatbot Mode Active ---\")\n",
    "while True:\n",
    "    u = input(\"\\nME: \").lower().strip()\n",
    "    if u in ['q', 'exit']: break\n",
    "    \n",
    "    # Adding a newline and space to 'prime' a fresh sentence\n",
    "    prompt = f\"me: {u}\\nai: \" \n",
    "    \n",
    "    # k=3 and temp=0.5 are usually the 'sweet spot' for this model size\n",
    "    full_output = generate(prompt, length=100, k=3, temp=0.5)\n",
    "    \n",
    "    # Get just the AI's part\n",
    "    response = full_output[len(prompt):]\n",
    "    \n",
    "    # 1. Stop if the model tries to roleplay as 'me:' again\n",
    "    response = response.split(\"me:\")[0].split(\"\\n\")[0]\n",
    "    \n",
    "    # 2. Fix 'Eaten' starts: If it starts with a fragment (like \"n't\"), \n",
    "    # find the next space and start from there.\n",
    "    if response.startswith(\"n't\") or response.startswith(\"'s\"):\n",
    "        parts = response.split(\" \", 1)\n",
    "        response = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "    # 3. Final Polish: remove trailing partial words\n",
    "    if response and response[-1] not in \" .!?\":\n",
    "        response = \" \".join(response.split(\" \")[:-1])\n",
    "\n",
    "    print(f\"AI: {response.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8475967",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_games = [\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.9, 0.9, 0.9], \n",
    "    [0.8, 0.2, 0.5],\n",
    "]\n",
    "results = [0, 0, 1]\n",
    "\n",
    "def knn_predict(new_data, dataset, targets, k=3):\n",
    "    distances = []\n",
    "    for i in range(len(dataset)):\n",
    "        dist = sum((new_data[j] - dataset[i][j])**2 for j in range(len(new_data)))**0.5\n",
    "        distances.append((dist, targets[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    nearest = [d[1] for d in distances[:k]]\n",
    "    return \"Хит\" if max(set(nearest), key=nearest.count) == 0 else \"Провал\"\n",
    "\n",
    "test_game = [0.15, 0.85, 0.12]\n",
    "print(f\"Вердикт KNN: {knn_predict(test_game, old_games, results, k=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afff137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_predict(game):\n",
    "    if game[1] > 0.8:\n",
    "        if game[0] < 0.3: return \"Инди-хит\"\n",
    "        else: return \"Блокбастер\"\n",
    "    else: return \"Провал\"\n",
    "\n",
    "new_game = [0.1, 0.9, 0.5]\n",
    "print(f\"Вердикт дерева: {tree_predict(new_game)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "dataset = [[0.1, 0.8, 0.1], [0.9, 0.9, 0.9], [0.8, 0.2, 0.5]]\n",
    "targets = [1, 1, -1] \n",
    "weights = [random.uniform(-0.1, 0.1) for _ in range(3)]\n",
    "bias = 0.0\n",
    "lr = 0.01 * 2\n",
    "C = 1.0\n",
    "epochs = 1000\n",
    "l1_param = 0.005\n",
    "l2_param = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, x in enumerate(dataset):\n",
    "        condition = targets[i] * (sum(x[j] * weights[j] for j in range(3)) + bias)\n",
    "        if condition >= 1:\n",
    "            for j in range(3):\n",
    "                weights[j] -= lr * (l2_param * weights[j] + l1_param * (1 if weights[j] > 0 else -1))\n",
    "        else:\n",
    "            for j in range(3):\n",
    "                weights[j] -= lr * (l2_param * weights[j] - C * x[j] * targets[i])\n",
    "            bias += lr * C * targets[i]\n",
    "\n",
    "test_game = [0.15, 0.85, 0.12]\n",
    "result = sum(test_game[j] * weights[j] for j in range(3)) + bias\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "prob_hit = sigmoid(result)\n",
    "print(f\"SVM вердикт: {'Хит' if result > 0 else 'Провал'}\")\n",
    "print(f\"Счет (Score): {result:.2f}\")\n",
    "print(f\"Уверенность (Вероятность Хима): {prob_hit:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
