{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import os\n",
    "with open('C:\\\\Users\\\\user\\\\Downloads\\\\BNCCorpus.txt', 'r', encoding='utf-8') as f:    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(name, tensor): print(f'debug [{name}]: shape={list(tensor.shape)} | mean={tensor.mean().item():.4f}')\n",
    "\n",
    "def get_batch(dataset, length, batch_size):\n",
    "    ix = torch.randint(len(dataset) - length - 2, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy(dataset[i:i+length]).long() for i in ix])\n",
    "    y1 = torch.stack([torch.from_numpy(dataset[i+1:i+length+1]).long() for i in ix])\n",
    "    y2 = torch.stack([torch.from_numpy(dataset[i+2:i+length+2]).long() for i in ix])\n",
    "    return x, y1, y2\n",
    "\n",
    "def solidify(model):\n",
    "    print(\"\\n-> SOLIDIFYING CORE: Migrating to INT8 for Ryzen 3...\")\n",
    "    model.eval()\n",
    "    qmodel = torch.ao.quantization.quantize_dynamic(\n",
    "        model, {nn.Linear}, dtype=torch.qint8)\n",
    "    return qmodel\n",
    "\n",
    "def print_sovereign_report(step, loss, experts_usage):\n",
    "    print(f\"\\n--- SOVEREIGN PERFORMANCE REPORT ---\")\n",
    "    print(f\"Step: {step} | Convergence: {1/loss:.4f}\")\n",
    "    print(f\"Expert Balance (Entropy): {np.std(experts_usage):.2f}\")\n",
    "    print(f\"Hardware Status: Stable (Ryzen 3 Optimized)\")\n",
    "    print(f\"------------------------------------\\n\")\n",
    "\n",
    "def loadbalance(routes, experts):\n",
    "    valid_routes = [r for r in routes if r is not None]\n",
    "    if not valid_routes: return 0\n",
    "\n",
    "    allroutes = torch.cat([r.view(-1, experts) for r in valid_routes])\n",
    "    importance = allroutes.mean(dim=0)\n",
    "\n",
    "    loss = experts * torch.sum(importance**2) - 1\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029183a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bpe:\n",
    "    def __init__(self, v):\n",
    "        self.v = v\n",
    "        self.merges = {}\n",
    "        self.vocab = {i: bytes([i]) for i in range(256)}\n",
    "        self.cache = {}\n",
    "        self.split_pattern = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "    def train(self, text):\n",
    "        word_chunks = re.findall(self.split_pattern, text)\n",
    "        word_counts = {}\n",
    "        for word in word_chunks:\n",
    "            tokens = tuple(word.encode('utf-8'))\n",
    "            word_counts[tokens] = word_counts.get(tokens, 0) + 1\n",
    "\n",
    "        for v in range(self.v - 256):\n",
    "            stats = {}\n",
    "            for word_tokens, freq in word_counts.items():\n",
    "                for pair in zip(word_tokens, word_tokens[1:]):\n",
    "                    stats[pair] = stats.get(pair, 0) + freq\n",
    "            if not stats: break\n",
    "            \n",
    "            max_pair = max(stats, key=stats.get)\n",
    "            idx = 256 + v\n",
    "            self.merges[max_pair] = idx\n",
    "            self.vocab[idx] = self.vocab[max_pair[0]] + self.vocab[max_pair[1]]\n",
    "\n",
    "            new_word_counts = {}\n",
    "            for word_tokens, freq in word_counts.items():\n",
    "                new_tokens = self._merge(word_tokens, max_pair, idx)\n",
    "                new_word_counts[new_tokens] = freq\n",
    "            word_counts = new_word_counts\n",
    "        \n",
    "    def _merge(self, tokens, pair, idx):\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n",
    "                new_tokens.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        return tuple(new_tokens)\n",
    "\n",
    "    def encode(self, text):\n",
    "        if text in self.cache: return self.cache[text]\n",
    "        word_chunks = re.findall(self.split_pattern, text)\n",
    "        final_ids = []\n",
    "        for word in word_chunks:\n",
    "            final_ids.extend(self._encode_word(word))\n",
    "        \n",
    "        self.cache[text] = final_ids\n",
    "        return final_ids\n",
    "\n",
    "    def _encode_word(self, word):\n",
    "        tokens = list(word.encode('utf-8'))\n",
    "        while len(tokens) >= 2:\n",
    "            stats = {p: self.merges[p] for p in zip(tokens, tokens[1:]) if p in self.merges}\n",
    "            if not stats: break\n",
    "            pair = min(stats.keys(), key=lambda p: self.merges[p])\n",
    "            tokens = list(self._merge(tokens, pair, self.merges[pair]))\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return b\"\".join(self.vocab.get(i, b\"<?>\") for i in ids).decode('utf-8', errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rms(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) + self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e57fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mrrope(nn.Module):\n",
    "    def __init__(self, q, k, z):\n",
    "        super().__init__()\n",
    "        self.q = q\n",
    "        self.k = k\n",
    "        self.z = z\n",
    "    \n",
    "    def rotatehalf(self, x):\n",
    "        x1, x2 = x.chunk(2, dim=-1)\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "    \n",
    "    def apply(self, q, k):\n",
    "        T = q.shape[1]\n",
    "        dim = q.shape[-1]\n",
    "        rates = torch.exp(torch.linspace(0, -np.log(10000), dim // 2)).to(q.device)\n",
    "        t = torch.arange(T, device=q.device).float()\n",
    "        freqs = torch.outer(t, rates)\n",
    "        cos = torch.cos(freqs).repeat(1, 2)\n",
    "        sin = torch.sin(freqs).repeat(1, 2)\n",
    "        q = (q * cos) + (self.rotatehalf(q) * sin)\n",
    "        k = (k * cos) + (self.rotatehalf(k) * sin)\n",
    "        return q, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hashedmla(nn.Module):\n",
    "    def __init__(self, emb, nbuckets):\n",
    "        super().__init__()\n",
    "        self.kv_dim = emb // 4\n",
    "        self.nbuckets = nbuckets\n",
    "        self.lshproj = nn.Parameter(torch.randn(self.kv_dim, nbuckets))\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        q_b = torch.argmax(q @ self.lshproj, dim=-1)\n",
    "        k_b = torch.argmax(k @ self.lshproj, dim=-1)\n",
    "\n",
    "        mask = (q_b.unsqueeze(-1) == k_b.unsqueeze(-2))\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1) * (self.kv_dim ** -0.5))\n",
    "        attn = attn.masked_fill(~mask, -1e9)\n",
    "\n",
    "        weights = F.softmax(attn, dim=-1)\n",
    "        return weights @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class moe(nn.Module):\n",
    "    def __init__(self, emb, experts):\n",
    "        super().__init__()\n",
    "        self.experts = experts\n",
    "        kv_dim = emb // 4\n",
    "        self.w_q = nn.Parameter(torch.randn(experts, emb, kv_dim))\n",
    "        self.w_o = nn.Parameter(torch.randn(experts, kv_dim, emb))\n",
    "    \n",
    "    def forward(self, x, route_soft):\n",
    "        q_experts = torch.einsum('bti,eio->bteo', x, self.w_q)\n",
    "        out_experts = torch.einsum('bteo,eok->btek', q_experts, self.w_o)\n",
    "        out = (out_experts * route_soft.unsqueeze(-1)).sum(dim=2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lnn(nn.Module):\n",
    "    def __init__(self, emb):\n",
    "        super().__init__()\n",
    "        self.dt = nn.Parameter(torch.randn(emb))\n",
    "        self.ambient = nn.Parameter(torch.ones(1))\n",
    "        self.tau = nn.Parameter(torch.ones(emb))\n",
    "\n",
    "    def forward(self, h, combined_context):\n",
    "        dt = self.dt * torch.sigmoid(self.ambient)\n",
    "        dh_dt = (-h + combined_context) / torch.exp(self.tau)\n",
    "        h_liquid = h + dh_dt * dt\n",
    "        return h_liquid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aec125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class swigluznn(nn.Module):\n",
    "    def __init__(self, emb):\n",
    "        super().__init__()\n",
    "        self.wgate = nn.Linear(emb, int(8/3 * emb))\n",
    "        self.wup = nn.Linear(emb, int(8/3 * emb))\n",
    "        self.wdown = nn.Linear(int(8/3 * emb), emb)\n",
    "\n",
    "    def forward(self, hin, h_liquid):\n",
    "        gate = self.wgate(h_liquid)\n",
    "        up = self.wup(h_liquid)\n",
    "        hff = F.silu(gate) * up\n",
    "        h = hin + self.wdown(hff)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447efae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lru(nn.Module):\n",
    "    def __init__(self, emb):\n",
    "        super().__init__()\n",
    "        self.cache = []\n",
    "        self.lrugate = nn.Linear(emb, emb)\n",
    "\n",
    "    def forward(self, lrustate, hnorm):\n",
    "        lambda_lru = torch.sigmoid(self.lrugate(hnorm))\n",
    "        lru_expanded = lrustate.unsqueeze(1).expand(-1, hnorm.size(1), -1)\n",
    "        h_mem = lambda_lru * hnorm + (1 - lambda_lru) * lru_expanded\n",
    "        return h_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6045f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class heads(nn.Module):\n",
    "    def __init__(self, emb, vocab):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab, emb)\n",
    "        self.pred_state = nn.Linear(emb, emb)\n",
    "        self.head_t1 = nn.Linear(emb, emb, bias=False)\n",
    "        self.head_t2 = nn.Linear(emb, emb, bias=False)\n",
    "        self.norm_f = rms(emb)\n",
    "        self.tw = self.embed.weight\n",
    "\n",
    "    def forward(self, h_final, i4=None):\n",
    "        h_final = self.norm_f(h_final)\n",
    "        l1 = self.head_t1(h_final)\n",
    "        ph = self.pred_state(h_final)\n",
    "        l2 = None\n",
    "        if i4 is not None:\n",
    "            l2 = self.head_t2(self.norm_f(i4))\n",
    "        return l1, l2, ph, h_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer(nn.Module):\n",
    "    def __init__(self, i, emb, experts):\n",
    "        super().__init__()\n",
    "        self.i = i\n",
    "        self.lru = lru(emb)\n",
    "        self.lnn = lnn(emb)\n",
    "        self.norm = rms(emb)\n",
    "        self.mrrope = mrrope(emb // 8, emb // 8, emb)\n",
    "\n",
    "        if i % 3 == 0:\n",
    "            self.is_expert = True\n",
    "            self.branch = moe(emb, experts)\n",
    "            self.router = nn.Linear(emb, experts)\n",
    "        else:\n",
    "            self.is_expert = False\n",
    "            self.branch = swigluznn(emb)\n",
    "\n",
    "    def forward(self, h, lrustate, route=None):\n",
    "        hnorm = self.norm(h)\n",
    "        h_mem = self.lru(lrustate, hnorm)\n",
    "        h_mem, _ = self.mrrope.apply(h_mem, h_mem)\n",
    "        if self.is_expert:\n",
    "            combined_context = self.branch(h_mem, route)\n",
    "        else:\n",
    "            combined_context = self.branch(h, h_mem) \n",
    "        h_liquid = self.lnn(h_mem, combined_context)\n",
    "        new_lru_state = h_liquid[:, -1, :].detach() \n",
    "        return h + h_liquid, new_lru_state, route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, vocab, emb, n_layers, n_experts):\n",
    "        super().__init__()\n",
    "        self.emb = emb\n",
    "        self.vocab = vocab\n",
    "        self.n_experts = n_experts\n",
    "        self.layers = nn.ModuleList([layer(i, emb, n_experts) for i in range(n_layers)])\n",
    "        self.heads = heads(emb, vocab)\n",
    "\n",
    "    def forward(self, x, lru=None, step=0, tau=1.0):\n",
    "        h = self.embed(x)\n",
    "        if lru is None: lru = torch.zeros(x.size(0), self.emb, device=x.device)\n",
    "        routes = []\n",
    "\n",
    "        for i, l in enumerate(self.layers):\n",
    "            route = None\n",
    "            if l.is_expert:\n",
    "                logits = l.router(h)\n",
    "                if self.training and step < 500:\n",
    "                    route = torch.ones_like(logits) / self.n_experts\n",
    "                else:\n",
    "                    route = F.gumbel_softmax(logits, tau=tau, hard=True)\n",
    "                routes.append(route)\n",
    "            h, lru_new, route = l(h, lru, route)\n",
    "            lru = lru_new \n",
    "            if i == 1: i4 = h.clone()\n",
    "\n",
    "        l1, l2, ph, h_final = self.heads(h, i4)\n",
    "        return l1, l2, ph, h_final, lru, routes\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class generate(nn.Module):\n",
    "    def __init__(self, model, tokenizer, length, tmp, topk, topp):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = length\n",
    "        self.tmp = tmp\n",
    "        self.topk = topk\n",
    "        self.topp = topp\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, prompt):\n",
    "        self.eval()\n",
    "        device = next(self.model.parameters()).device\n",
    "        tokens = torch.tensor([self.tokenizer.encode(prompt)], dtype=torch.long)\n",
    "        tokens = tokens.to(device)\n",
    "        lru_state = None\n",
    "        print('\\n')\n",
    "        print('model: ', end='', flush=True)\n",
    "        for _ in range(self.length):\n",
    "            logits1, logits2, _, _, _, _ = self.model(tokens, lru=lru_state)\n",
    "\n",
    "            next_token_logits = logits1[:, -1, :] / self.tmp\n",
    "            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > self.topp\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            next_token_logits[indices_to_remove] = float('-inf')\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            spec1 = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            if logits2 is not None: \n",
    "                spec2 = logits2[:, -1, :].argmax(-1, keepdim=True)\n",
    "                val1, _, _, _, _, _ = self.model.forward(spec1, lru=lru_state)\n",
    "                val2 = val1[:, -1, :].argmax(-1, keepdim=True)\n",
    "                if spec2 == val2:\n",
    "                    tokens = torch.cat([tokens, spec1, spec2], dim=1)\n",
    "                    print(self.tokenizer.decode(spec1[0].tolist()) + \n",
    "                          self.tokenizer.decode(spec2[0].tolist()), end='', flush=True)\n",
    "                    tokens = spec2\n",
    "                    continue\n",
    "            print(self.tokenizer.decode(spec1[0].tolist()), end='', flush=True)\n",
    "            tokens = spec1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8faa131",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\user\\\\Downloads\\\\BNCCorpus.txt'\n",
    "tokens = 'C:\\\\Users\\\\user\\\\Downloads\\\\tokenized_data.npy'\n",
    "vocab, emb, layers, experts = 2048, 128, 3, 4\n",
    "block, batch, baselr, minlr = 64, 8, 5e-4, 5e-5\n",
    "inittau, mintau, steps, warmup = 1.0, 0.1, 401, 200\n",
    "\n",
    "tokenizer = bpe(vocab)\n",
    "if os.path.exists(tokens):\n",
    "    token_data = np.load(tokens)\n",
    "    print(\"Loaded pre-tokenized data.\")\n",
    "else:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f: \n",
    "        text = f.read().lower()[:100000]\n",
    "    print(\"Tokenizing text (this may take a few minutes once)...\")\n",
    "    token_data = np.array(tokenizer.encode(text), dtype=np.uint16)\n",
    "    np.save(tokens, token_data)\n",
    "    print(\"Tokens saved.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt = model(vocab, emb, layers, experts).to(device)\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "scaler = torch.amp.GradScaler('cuda') if torch.cuda.is_available() else None\n",
    "\n",
    "checkpoint_path = \"sovereign_core.pt\"\n",
    "print(\"\\n[SOVEREIGN CORE MANAGEMENT]\")\n",
    "action = input(\"Command: [c]ontinue, [i]nterface/chat, or [d]elete? \").lower()\n",
    "\n",
    "if action == 'd':\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        os.remove(checkpoint_path)\n",
    "        print(\"!!! CORE PURGED: Starting fresh. !!!\")\n",
    "    start_step = 0\n",
    "    mode = 'train'\n",
    "\n",
    "elif action == 'i':\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"--- Loading Sovereign for Dialogue... ---\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        gpt.load_state_dict(checkpoint['model_state_dict']) \n",
    "        gpt.eval()\n",
    "        mode = 'chat'\n",
    "    else:\n",
    "        print(\"!! No Core found to chat with. Please train first. !!\")\n",
    "        exit()\n",
    "\n",
    "elif action == 'c':\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        gpt.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_step = checkpoint['step']\n",
    "        mode = 'train'\n",
    "        print(f\"-> Resuming training from step {start_step}\")\n",
    "    else:\n",
    "        print(\"? No checkpoint found. Starting fresh.\")\n",
    "        start_step = 0\n",
    "        mode = 'train'\n",
    "else:\n",
    "    print(\"--- Initializing New Training ---\")\n",
    "    start_step = 0\n",
    "    mode = 'train'\n",
    "\n",
    "if mode == 'chat':\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\" SOVEREIGN INTERFACE ACTIVE \")\n",
    "    print(\" (Type 'exit' to close) \")\n",
    "    print(\"=\"*30)\n",
    "    generator = generate(gpt, tokenizer, length=50, tmp=0.7, topk=50, topp=0.9)\n",
    "    while True:\n",
    "        u = input(\"\\nYou: \").strip()\n",
    "        if u.lower() in ['exit', 'q']: break\n",
    "        generator(u)\n",
    "\n",
    "elif mode == 'train':\n",
    "    for i in range(start_step, steps):\n",
    "        if i < warmup:\n",
    "            curr_lr = baselr * (i / warmup)\n",
    "            curr_tau = inittau - (inittau - mintau) * (i / steps)\n",
    "        else:\n",
    "            progress = (i - warmup) / (steps - warmup)\n",
    "            curr_lr = minlr + 0.5 * (baselr - minlr) * (1 + np.cos(np.pi * progress))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = curr_lr\n",
    "        xb, yb1, yb2 = get_batch(token_data, block, batch)\n",
    "        xb, yb1, yb2 = xb.to(device), yb1.to(device), yb2.to(device)\n",
    "        \n",
    "        dtype = torch.float16 if torch.cuda.is_available() else torch.bfloat16\n",
    "        device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        with torch.amp.autocast(device_type=device_type, dtype=dtype):\n",
    "            l1, l2, ph, hf, _, routes = gpt(xb, step=i, tau=curr_tau)\n",
    "            loss_t1 = F.cross_entropy(l1.view(-1, vocab), yb1.view(-1))\n",
    "            if i > 200:\n",
    "                main_loss = F.cross_entropy(l1.view(-1, vocab), yb1.view(-1))\n",
    "                aux_loss = loadbalance(routes, experts)\n",
    "                dream_loss = F.mse_loss(ph[:, :-1, :], hf[:, 1:, :].detach())\n",
    "                total_loss = main_loss + 0.01 * aux_loss + dream_loss * 0.1\n",
    "            else:\n",
    "                total_loss = loss_t1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if scaler:\n",
    "            scaler.scale(total_loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(gpt.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(gpt.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        if i % 50 == 0: \n",
    "            valid = [r for r in routes if r is not None]\n",
    "            usage = valid[-1].sum(dim=(0, 1)).cpu().detach().float().numpy()\n",
    "            print_sovereign_report(i, total_loss.item(), usage)\n",
    "        if i % 200 == 0 and i > start_step:\n",
    "            temp_path = checkpoint_path + \".tmp\"\n",
    "            torch.save({\n",
    "                'step': i,\n",
    "                'model_state_dict': gpt.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': total_loss.item(),\n",
    "            }, temp_path)\n",
    "            os.replace(temp_path, checkpoint_path) \n",
    "print(\"\\n--- TRAINING COMPLETE ---\")\n",
    "actionf = input(\"Command: [q]uantize model or just [s]ave? \").lower()\n",
    "\n",
    "if actionf == 'q':\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        gpt = solidify(gpt)\n",
    "        torch.save(gpt.state_dict(), \"C:\\\\Users\\\\user\\\\Downloads\\\\sovereign_solid.pt\")\n",
    "        print(\"-> Final Core Solidified and Saved as 'sovereign_solid.pt'\")\n",
    "\n",
    "elif actionf == 's':\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        torch.save(gpt.state_dict(), \"C:\\\\Users\\\\user\\\\Downloads\\\\sovereign_solid.pt\")\n",
    "        print(\"-> Final Core Just Saved as 'sovereign_solid.pt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
