{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed5048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Results ---\n",
      "Word Index 0 | Target: [1, 0] | Probs: [0.637, 0.363]\n",
      "Word Index 1 | Target: [0, 1] | Probs: [0.637, 0.363]\n",
      "Word Index 2 | Target: [1, 0] | Probs: [0.637, 0.363]\n",
      "Word Index 3 | Target: [0, 1] | Probs: [0.637, 0.363]\n",
      "Word Index 4 | Target: [1, 0] | Probs: [0.637, 0.363]\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "\n",
    "vocab_size, embed_dim = 10, 4\n",
    "hidden_1, hidden_2 = 8, 4\n",
    "lr, l1_param, l2_param = 0.05, 0.001, 0.01\n",
    "margin, eps, mucoef, vecoef = 2.0, 1e-8, 0.9, 0.999\n",
    "\n",
    "embedding_table = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(vocab_size)]\n",
    "neurons = [[random.uniform(-0.1, 0.1) for _ in range(embed_dim)] for _ in range(hidden_1)]\n",
    "layer_2 = [[random.uniform(-0.1, 0.1) for _ in range(hidden_1)] for _ in range(hidden_2)]\n",
    "weights = [[random.uniform(-0.1, 0.1) for _ in range(hidden_2)] for _ in range(2)]\n",
    "\n",
    "n_bias, l2_bias, w_bias = [0.0]*hidden_1, [0.0]*hidden_2, [0.0]*2\n",
    "\n",
    "mu_e, ve_e = [[0.0]*embed_dim for _ in range(vocab_size)], [[0.0]*embed_dim for _ in range(vocab_size)]\n",
    "mu_n, ve_n = [[0.0]*embed_dim for _ in range(hidden_1)], [[0.0]*embed_dim for _ in range(hidden_1)]\n",
    "mu_l2, ve_l2 = [[0.0]*hidden_1 for _ in range(hidden_2)], [[0.0]*hidden_1 for _ in range(hidden_2)]\n",
    "mu_w, ve_w = [[0.0]*hidden_2 for _ in range(2)], [[0.0]*hidden_2 for _ in range(2)]\n",
    "\n",
    "mu_nb, ve_nb = [0.0]*hidden_1, [0.0]*hidden_1\n",
    "mu_l2b, ve_l2b = [0.0]*hidden_2, [0.0]*hidden_2\n",
    "mu_wb, ve_wb = [0.0]*2, [0.0]*2\n",
    "dataset, target = [0, 1, 2, 3, 4], [[1, 0], [0, 1], [1, 0], [0, 1], [1, 0]]\n",
    "\n",
    "batch_size, batch_count = 3, 0\n",
    "acc_w = [[0.0]*hidden_2 for _ in range(2)]; acc_wb = [0.0]*2\n",
    "acc_l2 = [[0.0]*hidden_1 for _ in range(hidden_2)]; acc_l2b = [0.0]*hidden_2\n",
    "acc_n = [[0.0]*embed_dim for _ in range(hidden_1)]; acc_nb = [0.0]*hidden_1\n",
    "acc_e = [[0.0]*embed_dim for _ in range(vocab_size)]\n",
    "\n",
    "dropout = 0.2\n",
    "t = 0\n",
    "\n",
    "for epoch in range(500):\n",
    "    data_indices = list(range(len(dataset)))\n",
    "    random.shuffle(data_indices)\n",
    "    for batch_idx, i in enumerate(data_indices):\n",
    "        batch_count += 1\n",
    "        emb_idx = dataset[i]\n",
    "        vectors = embedding_table[emb_idx]\n",
    "\n",
    "        n_logits_pre = [max(0, sum(v * w for v, w in zip(vectors, row)) + n_bias[idx]) for idx, row in enumerate(neurons)]\n",
    "        n_mask = [1 if random.random() > dropout else 0 for _ in range(hidden_1)]\n",
    "        n_logits = [(val * m) / (1 - dropout) for val, m in zip(n_logits_pre, n_mask)]\n",
    "        l2_logits_pre = [max(0, sum(v * w for v, w in zip(n_logits, row)) + l2_bias[idx]) for idx, row in enumerate(layer_2)]\n",
    "        l2_mask = [1 if random.random() > dropout else 0 for _ in range(hidden_2)]\n",
    "        l2_logits = [(val * m) / (1 - dropout) for val, m in zip(l2_logits_pre, l2_mask)]\n",
    "        logits = [sum(v * w for v, w in zip(l2_logits, row)) + w_bias[idx] for idx, row in enumerate(weights)]\n",
    "        train_logits = list(logits)\n",
    "        correct_idx = target[i].index(1)\n",
    "        train_logits[correct_idx] /= margin\n",
    "        probs = [math.exp(l) / sum(math.exp(x) for x in train_logits) for l in train_logits]\n",
    "\n",
    "        w_loss = [probs[j] - target[i][j] for j in range(2)]\n",
    "        l2_loss = [sum(w_loss[k] * weights[k][j] for k in range(2)) * (1 if l2_logits_pre[j] > 0 else 0) * l2_mask[j] for j in range(hidden_2)]\n",
    "        n_loss = [sum(l2_loss[k] * layer_2[k][j] for k in range(hidden_2)) * (1 if n_logits_pre[j] > 0 else 0) * n_mask[j] for j in range(hidden_1)]\n",
    "        e_loss = [sum(n_loss[j] * neurons[j][k] for j in range(hidden_1)) for k in range(embed_dim)]\n",
    "        for j in range(2):\n",
    "            for k in range(hidden_2): acc_w[j][k] += w_loss[j] * l2_logits[k]\n",
    "            acc_wb[j] += w_loss[j]\n",
    "        for j in range(hidden_2):\n",
    "            for k in range(hidden_1): acc_l2[j][k] += l2_loss[j] * n_logits[k]\n",
    "            acc_l2b[j] += l2_loss[j]\n",
    "        for j in range(hidden_1):\n",
    "            for k in range(embed_dim): acc_n[j][k] += n_loss[j] * vectors[k]\n",
    "            acc_nb[j] += n_loss[j]\n",
    "        for k in range(embed_dim): acc_e[emb_idx][k] += e_loss[k]\n",
    "\n",
    "        if batch_count == batch_size or (batch_idx + 1) == len(dataset):\n",
    "            t += 1\n",
    "            n_samples = batch_count \n",
    "            \n",
    "            for j in range(2):\n",
    "                for k in range(hidden_2):\n",
    "                    grad = acc_w[j][k] / n_samples\n",
    "                    reg = l2_param * weights[j][k] + l1_param * (1 if weights[j][k] > 0 else -1)\n",
    "                    l2grad = grad + reg\n",
    "                    mu_w[j][k] = mucoef * mu_w[j][k] + (1 - mucoef) * l2grad\n",
    "                    ve_w[j][k] = vecoef * ve_w[j][k] + (1 - vecoef) * (l2grad**2)\n",
    "                    weights[j][k] -= lr * (mu_w[j][k]/(1-mucoef**t)) / (math.sqrt(ve_w[j][k]/(1-vecoef**t)) + eps)\n",
    "                \n",
    "                bias_grad = acc_wb[j] / n_samples\n",
    "                mu_wb[j] = mucoef * mu_wb[j] + (1 - mucoef) * bias_grad\n",
    "                ve_wb[j] = vecoef * ve_wb[j] + (1 - vecoef) * (bias_grad**2)\n",
    "                w_bias[j] -= lr * (mu_wb[j]/(1-mucoef**t)) / (math.sqrt(ve_wb[j]/(1-vecoef**t)) + eps)\n",
    "\n",
    "            for j in range(hidden_2):\n",
    "                for k in range(hidden_1):\n",
    "                    grad = acc_l2[j][k] / n_samples \n",
    "                    l2grad = grad + (l2_param * layer_2[j][k])\n",
    "                    mu_l2[j][k] = mucoef * mu_l2[j][k] + (1 - mucoef) * l2grad\n",
    "                    ve_l2[j][k] = vecoef * ve_l2[j][k] + (1 - vecoef) * (l2grad**2)\n",
    "                    layer_2[j][k] -= lr * (mu_l2[j][k]/(1-mucoef**t)) / (math.sqrt(ve_l2[j][k]/(1-vecoef**t)) + eps)\n",
    "                \n",
    "                bias_grad = acc_l2b[j] / n_samples\n",
    "                mu_l2b[j] = mucoef * mu_l2b[j] + (1 - mucoef) * bias_grad\n",
    "                ve_l2b[j] = vecoef * ve_l2b[j] + (1 - vecoef) * (bias_grad**2)\n",
    "                l2_bias[j] -= lr * (mu_l2b[j]/(1-mucoef**t)) / (math.sqrt(ve_l2b[j]/(1-vecoef**t)) + eps)\n",
    "\n",
    "            for j in range(hidden_1):\n",
    "                for k in range(embed_dim):\n",
    "                    grad = acc_n[j][k] / n_samples\n",
    "                    l2grad = grad + (l2_param * neurons[j][k])\n",
    "                    mu_n[j][k] = mucoef * mu_n[j][k] + (1 - mucoef) * l2grad\n",
    "                    ve_n[j][k] = vecoef * ve_n[j][k] + (1 - vecoef) * (l2grad**2)\n",
    "                    neurons[j][k] -= lr * (mu_n[j][k]/(1-mucoef**t)) / (math.sqrt(ve_n[j][k]/(1-vecoef**t)) + eps)\n",
    "                \n",
    "                bias_grad = acc_nb[j] / n_samples\n",
    "                mu_nb[j] = mucoef * mu_nb[j] + (1 - mucoef) * bias_grad\n",
    "                ve_nb[j] = vecoef * ve_nb[j] + (1 - vecoef) * (bias_grad**2)\n",
    "                n_bias[j] -= lr * (mu_nb[j]/(1-mucoef**t)) / (math.sqrt(ve_nb[j]/(1-vecoef**t)) + eps)\n",
    "\n",
    "            for idx in range(vocab_size):\n",
    "                for k in range(embed_dim):\n",
    "                    if any(acc_e[idx]):\n",
    "                        grad = acc_e[idx][k] / n_samples\n",
    "                        mu_e[idx][k] = mucoef * mu_e[idx][k] + (1 - mucoef) * grad\n",
    "                        ve_e[idx][k] = vecoef * ve_e[idx][k] + (1 - vecoef) * (grad**2)\n",
    "                        embedding_table[idx][k] -= lr * (mu_e[idx][k]/(1-mucoef**t)) / (math.sqrt(ve_e[idx][k]/(1-vecoef**t)) + eps)\n",
    "            \n",
    "            acc_w = [[0.0]*hidden_2 for _ in range(2)]; acc_wb = [0.0]*2\n",
    "            acc_l2 = [[0.0]*hidden_1 for _ in range(hidden_2)]; acc_l2b = [0.0]*hidden_2\n",
    "            acc_n = [[0.0]*embed_dim for _ in range(hidden_1)]; acc_nb = [0.0]*hidden_1\n",
    "            acc_e = [[0.0]*embed_dim for _ in range(vocab_size)]\n",
    "            batch_count = 0\n",
    "\n",
    "print(\"\\n--- Testing Results ---\")\n",
    "for i in range(len(dataset)):\n",
    "    emb_idx = dataset[i]\n",
    "    vectors = embedding_table[emb_idx]\n",
    "    n_l = [max(0, sum(v * w for v, w in zip(vectors, row)) + n_bias[idx]) for idx, row in enumerate(neurons)]\n",
    "    l2_l = [max(0, sum(v * w for v, w in zip(n_l, row)) + l2_bias[idx]) for idx, row in enumerate(layer_2)]\n",
    "    logits = [sum(v * w for v, w in zip(l2_l, row)) + w_bias[idx] for idx, row in enumerate(weights)]\n",
    "    exp_logits = [math.exp(l) for l in logits]\n",
    "    probs = [round(e / sum(exp_logits), 3) for e in exp_logits]\n",
    "    print(f\"Word Index {emb_idx} | Target: {target[i]} | Probs: {probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8475967",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_games = [\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.9, 0.9, 0.9], \n",
    "    [0.8, 0.2, 0.5],\n",
    "]\n",
    "results = [0, 0, 1]\n",
    "\n",
    "def knn_predict(new_data, dataset, targets, k=3):\n",
    "    distances = []\n",
    "    for i in range(len(dataset)):\n",
    "        dist = sum((new_data[j] - dataset[i][j])**2 for j in range(len(new_data)))**0.5\n",
    "        distances.append((dist, targets[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    nearest = [d[1] for d in distances[:k]]\n",
    "    return \"Хит\" if max(set(nearest), key=nearest.count) == 0 else \"Провал\"\n",
    "\n",
    "test_game = [0.15, 0.85, 0.12]\n",
    "print(f\"Вердикт KNN: {knn_predict(test_game, old_games, results, k=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afff137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_predict(game):\n",
    "    if game[1] > 0.8:\n",
    "        if game[0] < 0.3: return \"Инди-хит\"\n",
    "        else: return \"Блокбастер\"\n",
    "    else: return \"Провал\"\n",
    "\n",
    "new_game = [0.1, 0.9, 0.5]\n",
    "print(f\"Вердикт дерева: {tree_predict(new_game)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "dataset = [[0.1, 0.8, 0.1], [0.9, 0.9, 0.9], [0.8, 0.2, 0.5]]\n",
    "targets = [1, 1, -1] \n",
    "weights = [random.uniform(-0.1, 0.1) for _ in range(3)]\n",
    "bias = 0.0\n",
    "lr = 0.01 * 2\n",
    "C = 1.0\n",
    "epochs = 1000\n",
    "l1_param = 0.005\n",
    "l2_param = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, x in enumerate(dataset):\n",
    "        condition = targets[i] * (sum(x[j] * weights[j] for j in range(3)) + bias)\n",
    "        if condition >= 1:\n",
    "            for j in range(3):\n",
    "                weights[j] -= lr * (l2_param * weights[j] + l1_param * (1 if weights[j] > 0 else -1))\n",
    "        else:\n",
    "            for j in range(3):\n",
    "                weights[j] -= lr * (l2_param * weights[j] - C * x[j] * targets[i])\n",
    "            bias += lr * C * targets[i]\n",
    "\n",
    "test_game = [0.15, 0.85, 0.12]\n",
    "result = sum(test_game[j] * weights[j] for j in range(3)) + bias\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "prob_hit = sigmoid(result)\n",
    "print(f\"SVM вердикт: {'Хит' if result > 0 else 'Провал'}\")\n",
    "print(f\"Счет (Score): {result:.2f}\")\n",
    "print(f\"Уверенность (Вероятность Хима): {prob_hit:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
