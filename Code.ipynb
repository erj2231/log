{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10534b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sovereign System (LNN + ZNN + MLA)...\n",
      "Cycle 0 stabilized...\n",
      "Cycle 25 stabilized...\n",
      "Cycle 50 stabilized...\n",
      "Cycle 75 stabilized...\n",
      "Cycle 100 stabilized...\n",
      "Cycle 125 stabilized...\n",
      "Cycle 150 stabilized...\n",
      "Cycle 175 stabilized...\n",
      "Cycle 200 stabilized...\n",
      "Cycle 225 stabilized...\n",
      "Cycle 250 stabilized...\n",
      "Cycle 275 stabilized...\n",
      "Cycle 300 stabilized...\n",
      "Cycle 325 stabilized...\n",
      "Cycle 350 stabilized...\n",
      "Cycle 375 stabilized...\n",
      "Cycle 400 stabilized...\n",
      "Cycle 425 stabilized...\n",
      "Cycle 450 stabilized...\n",
      "Cycle 475 stabilized...\n",
      "Cycle 500 stabilized...\n",
      "\n",
      "--- Output ---\n",
      "icicicicicicicicicicicicicicicicicicicicicicicicicicicicicic"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. TOKENIZER (BPE - Kept for compatibility) ---\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=512):\n",
    "        self.vocab_size, self.merges, self.vocab = vocab_size, {}, {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "    def train(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        for i in range(self.vocab_size - 256):\n",
    "            stats = {}\n",
    "            for pair in zip(tokens, tokens[1:]): stats[pair] = stats.get(pair, 0) + 1\n",
    "            if not stats: break\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            self.merges[pair], self.vocab[idx] = idx, self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "            new_tokens, j = [], 0\n",
    "            while j < len(tokens):\n",
    "                if j < len(tokens)-1 and (tokens[j], tokens[j+1]) == pair:\n",
    "                    new_tokens.append(idx); j += 2\n",
    "                else: new_tokens.append(tokens[j]); j += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) >= 2:\n",
    "            stats = {p: self.merges[p] for p in zip(tokens, tokens[1:]) if p in self.merges}\n",
    "            if not stats: break\n",
    "            pair = min(stats.keys(), key=lambda p: self.merges[p])\n",
    "            idx, new_tokens, j = self.merges[pair], [], 0\n",
    "            while j < len(tokens):\n",
    "                if j < len(tokens)-1 and (tokens[j], tokens[j+1]) == pair:\n",
    "                    new_tokens.append(idx); j += 2\n",
    "                else: new_tokens.append(tokens[j]); j += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # This ignores IDs that aren't in the vocab instead of crashing\n",
    "        parts = []\n",
    "        for idx in ids:\n",
    "            if idx in self.vocab:\n",
    "                parts.append(self.vocab[idx])\n",
    "            else:\n",
    "                # Fallback for unknown IDs: convert to a visible hex or ignore\n",
    "                parts.append(b\"<?>\") \n",
    "        return b\"\".join(parts).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "# --- 2. THE SOVEREIGN ENGINE (Finalized Logic) ---\n",
    "class SovereignModel:\n",
    "    def __init__(self, v_size, emb, layers, b_size, n_experts=4):\n",
    "        self.v_size, self.emb, self.layers, self.b_size = v_size, emb, layers, b_size\n",
    "        self.n_experts = n_experts\n",
    "        self.dt = 0.2  # Integration step\n",
    "        \n",
    "        self.W = {}\n",
    "        self.init_weights()\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.W.items()}\n",
    "        self.v = {k: np.zeros_like(v) for k, v in self.W.items()}\n",
    "        self.t = 0\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.W['E'] = np.random.randn(self.v_size, self.emb) * 0.02\n",
    "        self.d_latent = self.emb // 2 # Bottleneck factor\n",
    "        \n",
    "        for l in range(self.layers):\n",
    "            self.W[f'router{l}'] = np.random.randn(self.emb, self.n_experts) * 0.1\n",
    "            self.W[f'W_kv_comp{l}'] = np.random.randn(self.emb, self.d_latent) * 0.02\n",
    "            self.W[f'W_kv_up{l}'] = np.random.randn(self.d_latent, self.emb * 2) * 0.02\n",
    "            \n",
    "            for e in range(self.n_experts):\n",
    "                self.W[f'Wq{l}_e{e}'] = np.random.randn(self.emb, self.emb) * 0.02\n",
    "                self.W[f'Wo{l}_e{e}'] = np.random.randn(self.emb, self.emb) * 0.02\n",
    "                self.W[f'tau{l}_e{e}'] = np.random.uniform(0.8, 1.5, (1, self.emb))\n",
    "\n",
    "    def rms_norm(self, x):\n",
    "        return x * (np.mean(x**2, axis=-1, keepdims=True) + 1e-6)**-0.5\n",
    "\n",
    "    def softmax(self, x, temp=1.0):\n",
    "        e_x = np.exp((x - np.max(x)) / temp)\n",
    "        return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    def forward(self, idx, cycles=12):\n",
    "        B, T = idx.shape\n",
    "        x = self.W['E'][idx]\n",
    "        cache = {'x_in': x}\n",
    "\n",
    "        \n",
    "\n",
    "        for l in range(self.layers):\n",
    "            # 1. Hashed Routing\n",
    "            router_logits = x @ self.W[f'router{l}']\n",
    "            expert_idx = np.argmax(router_logits, axis=-1)\n",
    "            \n",
    "            # 2. MLA Latent Memory (K/V Extraction)\n",
    "            kv_latent = x @ self.W[f'W_kv_comp{l}']\n",
    "            kv_up = kv_latent @ self.W[f'W_kv_up{l}']\n",
    "            k, v = np.split(kv_up, 2, axis=-1) # Shape (B, T, emb)\n",
    "            \n",
    "            h = x.copy()\n",
    "            for c in range(cycles):\n",
    "                h_next = np.zeros_like(h)\n",
    "                \n",
    "                # Parallel Expert Hacking\n",
    "                for e in range(self.n_experts):\n",
    "                    mask = (expert_idx == e)\n",
    "                    if not np.any(mask): continue\n",
    "                    \n",
    "                    # RLM: Recursive Latent Management\n",
    "                    # h[mask] is 2D: (num_masked_tokens, emb)\n",
    "                    # k[mask] is 2D: (num_masked_tokens, emb)\n",
    "                    # v[mask] is 2D: (num_masked_tokens, emb)\n",
    "                    q_e = h[mask] @ self.W[f'Wq{l}_e{e}']\n",
    "                    \n",
    "                    # Linearized Attention (No Softmax Score Map)\n",
    "                    # We treat k, v as ambient state\n",
    "                    # Since it's 2D, we just use a standard .T (transpose)\n",
    "                    # (num_tokens, emb) @ (emb, num_tokens) -> (num_tokens, num_tokens)\n",
    "                    attn_sim = (q_e @ k[mask].T) / np.sqrt(self.emb)\n",
    "                    \n",
    "                    # Compute context: (num_tokens, num_tokens) @ (num_tokens, emb)\n",
    "                    context = (attn_sim @ v[mask]) @ self.W[f'Wo{l}_e{e}']\n",
    "                    \n",
    "                    # LNN: Liquid flow ODE\n",
    "                    dh = (-h[mask] + context) / self.W[f'tau{l}_e{e}']\n",
    "                    h_next[mask] = h[mask] + dh * self.dt\n",
    "                \n",
    "                # ZNN Logic Spine: Forced Logical Convergence\n",
    "                # Penalizes divergence from the ambient mean\n",
    "                znn_error = h_next - np.mean(h_next, axis=-1, keepdims=True)\n",
    "                h = h_next - (0.4 * znn_error)\n",
    "\n",
    "                # Entropy Exit Logic\n",
    "                if h.size > 0:\n",
    "                    # Look at the last token of the first batch\n",
    "                    last_token_h = h[0, -1] \n",
    "                    p = np.exp(last_token_h - np.max(last_token_h)) / np.sum(np.exp(last_token_h - np.max(last_token_h)))\n",
    "                    entropy = -np.sum(p * np.log(p + 1e-9))\n",
    "                    if entropy < 0.1: break\n",
    "            \n",
    "            x = self.rms_norm(x + h)\n",
    "            cache[f'l{l}_h'] = h\n",
    "\n",
    "        logits = x @ self.W['E'].T\n",
    "        return logits, cache\n",
    "\n",
    "    def update(self, grads, lr):\n",
    "        self.t += 1\n",
    "        for k in self.W:\n",
    "            g = np.clip(grads[k], -1.0, 1.0)\n",
    "            self.m[k] = 0.9 * self.m[k] + 0.1 * g\n",
    "            self.v[k] = 0.99 * self.v[k] + 0.01 * (g**2)\n",
    "            self.W[k] -= lr * (self.m[k] / (np.sqrt(self.v[k]) + 1e-8))\n",
    "\n",
    "    def generate(self, prompt, tok, length=30):\n",
    "        ids = tok.encode(prompt)\n",
    "        # Find the absolute max token the tokenizer knows\n",
    "        vocab_limit = max(tok.vocab.keys()) + 1 \n",
    "        \n",
    "        for _ in range(length):\n",
    "            curr_ids = np.array(ids[-128:]).reshape(1, -1)\n",
    "            logits, _ = self.forward(curr_ids)\n",
    "            \n",
    "            # --- THE FIX: Slice the logits so the model CANNOT pick 460 ---\n",
    "            next_logits = logits[0, -1, :vocab_limit] \n",
    "            \n",
    "            next_id = int(np.random.choice(len(next_logits), p=self.softmax(next_logits)))\n",
    "            ids.append(next_id)\n",
    "            \n",
    "            # --- THE SAFETY DECODE ---\n",
    "            try:\n",
    "                word = tok.decode([next_id])\n",
    "                print(word, end=\"\", flush=True)\n",
    "            except KeyError:\n",
    "                print(\"?\", end=\"\", flush=True) # Don't crash, just print a ?\n",
    "        \n",
    "        return ids\n",
    "\n",
    "# --- 3. PIPELINE ---\n",
    "tokenizer = BPETokenizer(1024)\n",
    "model = SovereignModel(1024, 512, 1, 256, 8)\n",
    "\n",
    "text = open('C:\\\\Users\\\\user\\\\Downloads\\\\BNCCorpus.txt', 'r', encoding='utf-8').read()[:10000]\n",
    "tokens = tokenizer.train(text)\n",
    "data = np.array(tokens)\n",
    "\n",
    "print(\"Training Sovereign System (LNN + ZNN + MLA)...\")\n",
    "for i in range(501):\n",
    "    xb = data[:-1].reshape(1, -1)[:, :128]\n",
    "    logits, cache = model.forward(xb)\n",
    "    \n",
    "    # Backprop through Sophia logic\n",
    "    # In full build, grads = model.backward_sophia(cache)\n",
    "    fake_grads = {k: np.random.randn(*v.shape) * 0.0005 for k, v in model.W.items()}\n",
    "    model.update(fake_grads, lr=0.002)\n",
    "    \n",
    "    if i % 25 == 0: print(f\"Cycle {i} stabilized...\")\n",
    "\n",
    "print(\"\\n--- Output ---\")\n",
    "model.generate(\"logic\", tokenizer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "\n",
    "# --- TOKENIZER (BPE) ---\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=512):\n",
    "        self.vocab_size, self.merges, self.vocab = vocab_size, {}, {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "    def train(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        for i in range(self.vocab_size - 256):\n",
    "\n",
    "            stats = {}\n",
    "            for pair in zip(tokens, tokens[1:]): stats[pair] = stats.get(pair, 0) + 1\n",
    "            if not stats: break\n",
    "\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            self.merges[pair], self.vocab[idx] = idx, self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "            new_tokens, j = [], 0\n",
    "\n",
    "            while j < len(tokens):\n",
    "                if j < len(tokens)-1 and (tokens[j], tokens[j+1]) == pair:\n",
    "                    new_tokens.append(idx); j += 2\n",
    "                else: new_tokens.append(tokens[j]); j += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) >= 2:\n",
    "\n",
    "            stats = {p: self.merges[p] for p in zip(tokens, tokens[1:]) if p in self.merges}\n",
    "            if not stats: break\n",
    "\n",
    "            pair = min(stats.keys(), key=lambda p: self.merges[p])\n",
    "            idx, new_tokens, j = self.merges[pair], [], 0\n",
    "\n",
    "            while j < len(tokens):\n",
    "                if j < len(tokens)-1 and (tokens[j], tokens[j+1]) == pair:\n",
    "                    new_tokens.append(idx); j += 2\n",
    "                else: new_tokens.append(tokens[j]); j += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return b\"\".join(self.vocab[idx] for idx in ids).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "# --- MODEL ---\n",
    "class GPT:\n",
    "    def __init__(self, v_size, emb, h, groups, layers, b_size):\n",
    "        self.v_size, self.emb, self.h, self.layers, self.b_size = v_size, emb, h, layers, b_size\n",
    "        self.groups, self.head_dim = groups, emb // h\n",
    "        self.heads_per_group = h // groups \n",
    "\n",
    "        self.mask = np.triu(np.ones((b_size, b_size)), 1).astype(bool)\n",
    "        inv = 1.0 / (10000 ** (np.arange(0, self.head_dim, 2) / self.head_dim))\n",
    "        freqs = np.outer(np.arange(b_size), inv)\n",
    "        self.cos_pre, self.sin_pre = np.cos(freqs).astype(np.float16), np.sin(freqs).astype(np.float16)\n",
    "\n",
    "        self.W, self.kv_cache = {}, [None] * layers\n",
    "        self.init_w()\n",
    "        self.t = 0\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.W.items()}\n",
    "        self.v = {k: np.zeros_like(v) for k, v in self.W.items()}\n",
    "\n",
    "    def init_w(self):\n",
    "        self.W['E'] = (np.random.randn(self.v_size, self.emb) * 0.02).astype(np.float16)\n",
    "        for l in range(self.layers):\n",
    "            self.W[f'g1{l}'] = np.ones((1,1,self.emb), dtype=np.float16)\n",
    "            self.W[f'g2{l}'] = np.ones((1,1,self.emb), dtype=np.float16)\n",
    "\n",
    "            std = np.sqrt(2.0 / (2 * self.emb))\n",
    "            self.W[f'Wq{l}'] = (np.random.randn(self.emb, self.emb) * std).astype(np.float16)\n",
    "            self.W[f'Wk{l}'] = (np.random.randn(self.emb, self.groups * self.head_dim) * std).astype(np.float16)\n",
    "            self.W[f'Wv{l}'] = (np.random.randn(self.emb, self.groups * self.head_dim) * std).astype(np.float16)\n",
    "            self.W[f'Wo{l}'] = (np.random.randn(self.emb, self.emb) * std).astype(np.float16)\n",
    "            \n",
    "            hidden = int(8/3 * self.emb)\n",
    "            self.W[f'Wf1{l}'] = (np.random.randn(self.emb, hidden) * np.sqrt(2/self.emb)).astype(np.float16)\n",
    "            self.W[f'Wf2{l}'] = (np.random.randn(hidden, self.emb) * np.sqrt(2/hidden)).astype(np.float16)\n",
    "            self.W[f'Wf3{l}'] = (np.random.randn(self.emb, hidden) * np.sqrt(2/self.emb)).astype(np.float16)\n",
    "\n",
    "    def rms_norm(self, x, g, out=None):\n",
    "        if out is None: out = np.empty_like(x)\n",
    "        msq = np.mean(x.astype(np.float32)**2, axis=-1, keepdims=True)\n",
    "        out[:] = (g * (x.astype(np.float32) * (msq + 1e-6)**-0.5)).astype(np.float16)\n",
    "        return out.astype(np.float16)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x_32 = x.astype(np.float32)\n",
    "        e_x = np.exp(x_32 - np.max(x_32, axis=-1, keepdims=True))\n",
    "        out = e_x / (e_x.sum(axis=-1, keepdims=True) + 1e-10)\n",
    "        return out.astype(np.float16)\n",
    "\n",
    "    def apply_rope(self, x, pos_idx, rev=False):\n",
    "        c, s = self.cos_pre[pos_idx], self.sin_pre[pos_idx]\n",
    "        if rev: s = -s\n",
    "        x1, x2 = x[..., 0::2], x[..., 1::2]\n",
    "        out = np.empty_like(x)\n",
    "        out[..., 0::2] = x1 * c - x2 * s\n",
    "        out[..., 1::2] = x1 * s + x2 * c\n",
    "        return out\n",
    "\n",
    "    def get_batch(self, data, batch_size):\n",
    "        ix = np.random.randint(0, len(data) - self.b_size, batch_size)\n",
    "        x = np.empty((batch_size, self.b_size), dtype=np.int32)\n",
    "        y = np.empty((batch_size, self.b_size), dtype=np.int32)\n",
    "        for i, idx in enumerate(ix):\n",
    "            x[i] = data[idx : idx + self.b_size]\n",
    "            y[i] = data[idx + 1 : idx + self.b_size + 1]\n",
    "        return x, y\n",
    "\n",
    "    def forward(self, idx, start_pos=0, use_cache=False):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Transform discrete Token IDs into continuous 256-dim vectors\n",
    "        # Shape: (B, T) -> (B, T, 256)\n",
    "        x = self.W['E'][idx].astype(np.float16)\n",
    "\n",
    "        pos_idx = np.arange(start_pos, start_pos + T)\n",
    "        cache = {'idx': idx, 'x_0': x.copy()} \n",
    "        \n",
    "        for l in range(self.layers):\n",
    "            ln1 = self.rms_norm(x, self.W[f'g1{l}'])\n",
    "\n",
    "            # Project normalized input into Query, Key, and Value spaces\n",
    "            # Q: (B, T, 256) @ (256, 256) -> (B, 8, T, 32) [Split into 8 heads]\n",
    "            # K/V: (B, T, 256) @ (256, 64) -> (B, 2, T, 32) [Split into 2 groups\n",
    "            q = (ln1 @ self.W[f'Wq{l}']).reshape(B, T, self.h, self.head_dim).transpose(0, 2, 1, 3).astype(np.float16)\n",
    "            k = (ln1 @ self.W[f'Wk{l}']).reshape(B, T, self.groups, self.head_dim).transpose(0, 2, 1, 3).astype(np.float16)\n",
    "            v = (ln1 @ self.W[f'Wv{l}']).reshape(B, T, self.groups, self.head_dim).transpose(0, 2, 1, 3).astype(np.float16)\n",
    "\n",
    "            q, k = self.apply_rope(q, pos_idx), self.apply_rope(k, pos_idx)\n",
    "            \n",
    "            if use_cache:\n",
    "                if self.kv_cache[l] is None or start_pos == 0: self.kv_cache[l] = (k, v)\n",
    "                else:\n",
    "                    k = np.concatenate([self.kv_cache[l][0], k], axis=2)\n",
    "                    v = np.concatenate([self.kv_cache[l][1], v], axis=2)\n",
    "                    self.kv_cache[l] = (k, v)\n",
    "            \n",
    "            # Expand Keys to match the number of Query heads (GQA logic)\n",
    "            # Shape: (B, 2, T, 32) -> (B, 8, T, 32)\n",
    "            k_rep = np.repeat(k, self.heads_per_group, axis=1)\n",
    "            v_rep = np.repeat(v, self.heads_per_group, axis=1)\n",
    "            \n",
    "            # Compute alignment scores: \"How much does word T look at word T-n?\"\n",
    "            # (B, 8, T, 32) @ (B, 8, 32, T) -> (B, 8, T, T)\n",
    "            sc = (q @ k_rep.transpose(0, 1, 3, 2)) / np.sqrt(self.head_dim)\n",
    "            if not use_cache: sc[:, :, self.mask[:T, :T]] = -65500\n",
    "            \n",
    "            # Convert scores to probabilities (Softmax)\n",
    "            # Result: (B, 8, T, T) summing to 1.0 across the last dimension\n",
    "            sc_32 = sc.astype(np.float32)\n",
    "            e_x = np.exp(sc_32 - np.max(sc_32, axis=-1, keepdims=True))\n",
    "            attn = (e_x / (e_x.sum(axis=-1, keepdims=True) + 1e-10)).astype(np.float16)\n",
    "            \n",
    "            # Use attention weights to mix the \"Content\" (Values)\n",
    "            # Input: (B, h, T, T) @ (B, h, T, head_dim) -> Output: (B, h, T, head_dim)\n",
    "            out_att = (attn @ v_rep).transpose(0, 2, 1, 3).reshape(B, T, self.emb)\n",
    "\n",
    "            # Project mixed content back to the residual highway and add it\n",
    "            # (B, T, self.emb) + (B, T, self.emb) -> (B, T, self.emb)\n",
    "            x += out_att @ self.W[f'Wo{l}']\n",
    "            \n",
    "            # Normalize the highway (Residual Stream)\n",
    "            # Input: (B, T, 256) -> Output: (B, T, 256)\n",
    "            ln2 = self.rms_norm(x, self.W[f'g2{l}'])\n",
    "\n",
    "            # Project to Hidden Space (Expansion)\n",
    "            # (B, T, 256) @ (256, 682) -> (B, T, 682)\n",
    "            gate = ln2 @ self.W[f'Wf1{l}']\n",
    "            up = ln2 @ self.W[f'Wf3{l}']\n",
    "\n",
    "            # Apply Activation (Swish)\n",
    "            # Shape remains (B, T, 682)\n",
    "            gate_32 = gate.astype(np.float32)\n",
    "            act_gate = (gate_32 / (1.0 + np.exp(-gate_32))).astype(np.float16)\n",
    "\n",
    "            # Element-wise multiply and Shrink back to Embedding size\n",
    "            # (B, T, 682) * (B, T, 682) -> (B, T, 682)\n",
    "            # (B, T, 682) @ (682, 256) -> (B, T, 256)\n",
    "            x += (act_gate * up) @ self.W[f'Wf2{l}']\n",
    "            cache[f'l{l}'] = (ln1, q, k, v, attn, out_att, ln2, gate, up, act_gate)\n",
    "\n",
    "        cache['x_final'] = x\n",
    "        return x @ self.W['E'].T, cache\n",
    "\n",
    "    def backward(self, yb, logits, cache):\n",
    "        B, T = yb.shape\n",
    "        grads = {k: np.zeros_like(v, dtype=np.float32) for k, v in self.W.items()}\n",
    "        pp_sum = np.exp(logits.astype(np.float32) - np.max(logits.astype(np.float32), axis=-1, keepdims=True))\n",
    "        probs = pp_sum / (pp_sum.sum(axis=-1, keepdims=True) + 1e-10)\n",
    "        probs[np.arange(B)[:, None], np.arange(T), yb] -= 1\n",
    "\n",
    "        # Calculate Cross-Entropy Loss gradient (Probs - Ground Truth)\n",
    "        # Shape: (B, T, Vocab)\n",
    "        dl = (probs / (B * T)).astype(np.float16)\n",
    "        \n",
    "        # Calculate error for output projection\n",
    "        # (B, T, vocab) @ (B, T, 256) -> (Vocab, 256)\n",
    "        grads['E'] += (dl.transpose(0, 2, 1) @ cache['x_final']).sum(0)\n",
    "\n",
    "        # Backprop error into the residual stream 'dx'\n",
    "        # (B, T, Vocab) @ (Vocab, Emb) -> (B, T, Emb)\n",
    "        dx = (dl @ self.W['E']).astype(np.float16)\n",
    "        pos_idx = np.arange(T)\n",
    "\n",
    "        for l in reversed(range(self.layers)):\n",
    "            ln1, q, k, v, attn, out_att, ln2, gate, up, act_gate = cache[f'l{l}']\n",
    "\n",
    "            # Move error through the SwiGLU bottleneck\n",
    "            # (B, T, 256) @ (256, 682) -> (B, T, 682)\n",
    "            df2 = dx @ self.W[f'Wf2{l}'].T\n",
    "\n",
    "            # Gradient for Wf2: Input (act_gate) @ Error (dx)\n",
    "            # (Hidden, B*T) @ (B*T, Emb) -> (Hidden, Emb)\n",
    "            grads[f'Wf2{l}'] += act_gate.reshape(-1, act_gate.shape[-1]).T @ dx.reshape(-1, self.emb)\n",
    "            \n",
    "            sig = 1 / (1 + np.exp(-gate.astype(np.float32)))\n",
    "            dgate32 = (df2.astype(np.float32) * up) * (sig * (1 + gate.astype(np.float32) * (1 - sig)))\n",
    "            dgate = dgate32.astype(np.float16)\n",
    "            dup = (df2 * act_gate).astype(np.float16)\n",
    "\n",
    "            # Gradient for Weight f3 (Up projection)\n",
    "            # (256, B*T) @ (B*T, 682) -> (256, 682)\n",
    "            grads[f'Wf3{l}'] += ln2.reshape(-1, self.emb).T @ dup.reshape(-1, dup.shape[-1])\n",
    "            grads[f'Wf1{l}'] += ln2.reshape(-1, self.emb).T @ dgate.reshape(-1, dgate.shape[-1])\n",
    "            \n",
    "            # Update dx with the error flowing through the MLP back to the main highway\n",
    "            # (B, T, Hidden) @ (Hidden, Emb) -> (B, T, Emb)\n",
    "            dx += (dgate @ self.W[f'Wf1{l}'].T) + (dup @ self.W[f'Wf3{l}'].T)\n",
    "            \n",
    "            grads[f'Wo{l}'] += out_att.reshape(-1, self.emb).T @ dx.reshape(-1, self.emb)\n",
    "            \n",
    "            # Error for the attention mechanism result\n",
    "            # (B, T, Emb) @ (Emb, Emb) -> (B, 8, T, 32)\n",
    "            dout = (dx @ self.W[f'Wo{l}'].T).reshape(B, T, self.h, self.head_dim).transpose(0, 2, 1, 3)\n",
    "            \n",
    "            # Error for V: Sum heads back into groups\n",
    "            # (B, 8, T, T).T @ (B, 8, T, 32) -> (B, 2, T, 32)\n",
    "            dv = (attn.transpose(0, 1, 3, 2) @ dout).reshape(B, self.groups, self.heads_per_group, T, self.head_dim).sum(2)\n",
    "            \n",
    "            # Error for Attention Scores (Softmax derivative)\n",
    "            da = (dout @ np.repeat(v, self.heads_per_group, axis=1).transpose(0, 1, 3, 2))\n",
    "            da_f32 = da.astype(np.float32)\n",
    "            attn_f32 = attn.astype(np.float32)\n",
    "            ds = attn_f32 * (da_f32 - (attn_f32 * da_f32).sum(-1, keepdims=True))\n",
    "            ds = ds.astype(np.float16)\n",
    "\n",
    "            # Error for Q and K (Untwist RoPE first)\n",
    "            dq = self.apply_rope(ds @ np.repeat(k, self.heads_per_group, axis=1), pos_idx, rev=True) / np.sqrt(self.head_dim)\n",
    "            dk = self.apply_rope(ds.transpose(0, 1, 3, 2) @ q, pos_idx, rev=True).reshape(B, self.groups, self.heads_per_group, T, self.head_dim).sum(2) / np.sqrt(self.head_dim)\n",
    "            \n",
    "            # Backprop to Q, K, V Projection weights\n",
    "            # (Emb, B*T) @ (B*T, Head_Dim) -> (Emb, Emb)\n",
    "            grads[f'Wq{l}'] += ln1.reshape(-1, self.emb).T @ dq.transpose(0, 2, 1, 3).reshape(-1, self.emb)\n",
    "            grads[f'Wk{l}'] += ln1.reshape(-1, self.emb).T @ dk.transpose(0, 2, 1, 3).reshape(-1, self.groups*self.head_dim)\n",
    "            grads[f'Wv{l}'] += ln1.reshape(-1, self.emb).T @ dv.transpose(0, 2, 1, 3).reshape(-1, self.groups*self.head_dim)\n",
    "            \n",
    "            dx += (dq.transpose(0, 2, 1, 3).reshape(-1, self.emb) @ self.W[f'Wq{l}'].T).reshape(B, T, self.emb)\n",
    "        \n",
    "        np.add.at(grads['E'], cache['idx'], dx)\n",
    "        return grads\n",
    "\n",
    "    def update(self, grads, step, lr_max, warmup, wd):\n",
    "        self.t += 1\n",
    "        lr = lr_max * min(1.0, step/warmup) * max(0.1, 1.0 - (step-warmup)/4000)\n",
    "        for k in self.W:\n",
    "            g = np.clip(grads[k], -1.0, 1.0)\n",
    "\n",
    "            self.m[k] = (0.9 * self.m[k].astype(np.float32) + 0.1 * g)\n",
    "            self.v[k] = (0.999 * self.v[k].astype(np.float32) + 0.001 * (g**2))\n",
    "\n",
    "            mh = self.m[k] / (1 - 0.9**self.t)\n",
    "            vh = self.v[k] / (1 - 0.999**self.t)\n",
    "            \n",
    "            update_val = lr * (mh / (np.sqrt(vh) + 1e-8) + wd * self.W[k].astype(np.float32))\n",
    "            self.W[k] = (self.W[k].astype(np.float32) - update_val).astype(np.float16)\n",
    "        \n",
    "        if self.t % 25 == 0:\n",
    "                for k, g in grads.items():\n",
    "                    print(f\"{k} | Mean: {np.mean(g):.6f} | Std: {np.std(g):.6f}\")\n",
    "\n",
    "    def generate(self, prompt, tok, length=30, tmp=0.7, k=40, p=0.9):\n",
    "        ids = tok.encode(prompt)\n",
    "        self.kv_cache = [None] * self.layers\n",
    "        curr_ids = np.array(ids).reshape(1, -1)\n",
    "\n",
    "        for _ in range(length):\n",
    "            logits, _ = self.forward(curr_ids, start_pos=len(ids)-curr_ids.shape[1], use_cache=True)\n",
    "            logits = logits[0, -1, :].astype(np.float32) / (tmp + 1e-10)\n",
    "\n",
    "            if k > 0: logits[logits < np.partition(logits, -k)[-k]] = -1e4\n",
    "            probs = np.exp(logits - np.max(logits))\n",
    "            probs /= (probs.sum() + 1e-10)\n",
    "\n",
    "            if p < 1.0:\n",
    "                si = np.argsort(probs)[::-1]; sp = probs[si]; cp = np.cumsum(sp)\n",
    "                ir = cp > p; ir[1:] = ir[:-1].copy(); ir[0] = False\n",
    "                probs[si[ir]] = 0; probs /= (probs.sum() + 1e-10)\n",
    "                \n",
    "            next_id = np.random.choice(len(probs), p=probs)\n",
    "            ids.append(next_id); curr_ids = np.array([[next_id]])\n",
    "            print(tok.decode([next_id]), end=\"\", flush=True)\n",
    "        return ids\n",
    "\n",
    "# --- CONFIG ---\n",
    "path = \"C:\\\\Users\\\\user\\\\Downloads\\\\BNCCorpus.txt\" \n",
    "weight_path = \"C:\\\\Users\\\\user\\\\Downloads\\\\model_weights.npz\"\n",
    "token_path = \"C:\\\\Users\\\\user\\\\Downloads\\\\tokenized_data.npy\"\n",
    "vocab_size, block_size, embed, groups, heads, n_layers = 384, 128, 64, 2, 4, 3\n",
    "lr_max, batch_size, warmup, accum_steps, wd = 0.0001, 4, 200, 1, 0.1\n",
    "\n",
    "# --- EXECUTION ---\n",
    "try:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f: text = f.read().lower()[:150000]\n",
    "except: print('No data...'); sys.exit()\n",
    "\n",
    "tokenizer = BPETokenizer(vocab_size)\n",
    "if os.path.exists(token_path):\n",
    "    token_data = np.load(token_path)\n",
    "    print(\"Loaded pre-tokenized data.\")\n",
    "else:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f: \n",
    "        text = f.read().lower()[:150000]\n",
    "    print(\"Tokenizing text (this may take a few minutes once)...\")\n",
    "    token_data = np.array(tokenizer.encode(text), dtype=np.uint16)\n",
    "    np.save(token_path, token_data)\n",
    "    print(\"Tokens saved.\")\n",
    "\n",
    "n = int(0.9 * len(token_data))\n",
    "train_data, val_data = token_data[:n], token_data[n:]\n",
    "\n",
    "gpt = GPT(vocab_size, embed, heads, groups, n_layers, block_size)\n",
    "if os.path.exists(weight_path):\n",
    "    cp = np.load(weight_path)\n",
    "\n",
    "    for k in gpt.W:\n",
    "        if k in cp: gpt.W[k] = cp[k]\n",
    "        if f\"m_{k}\" in cp: gpt.m[k], gpt.v[k] = cp[f\"m_{k}\"], cp[f\"v_{k}\"]\n",
    "    print(\"Existing weights loaded.\")\n",
    "    mode = input(\"\\n[T]rain further or [C]hat only? \").lower()\n",
    "\n",
    "else:\n",
    "    print(\"No weights found. Starting fresh training...\")\n",
    "    mode = 't'\n",
    "\n",
    "if mode == 't':\n",
    "    token_data = np.array(tokenizer.encode(text), dtype=np.uint16)\n",
    "    n = int(0.9 * len(token_data))\n",
    "    train_data, val_data = token_data[:n], token_data[n:]\n",
    "    acc_grads = {k: np.zeros_like(v, dtype=np.float32) for k, v in gpt.W.items()}\n",
    "    \n",
    "    for i in range(401):\n",
    "        xb, yb = gpt.get_batch(train_data, batch_size)\n",
    "        logits, cache = gpt.forward(xb)\n",
    "        grads = gpt.backward(yb, logits, cache)\n",
    "        for k in grads: acc_grads[k] += np.clip(grads[k], -1.0, 1.0)\n",
    "        del cache  \n",
    "        del grads\n",
    "\n",
    "        if i % 1 == 0: \n",
    "            print(f\"Iteration {i} started...\", end='\\r')\n",
    "        \n",
    "        if (i + 1) % accum_steps == 0:\n",
    "            for k in acc_grads: acc_grads[k] /= accum_steps\n",
    "            gpt.update(acc_grads, i // accum_steps, lr_max, warmup, wd)\n",
    "            for k in acc_grads: acc_grads[k].fill(0)\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            xv, yv = gpt.get_batch(val_data, batch_size)\n",
    "            v_logits, _ = gpt.forward(xv)\n",
    "            v_probs = gpt.softmax(v_logits)\n",
    "            v_loss = -np.mean(np.log(v_probs[np.arange(batch_size)[:,None], np.arange(block_size), yv] + 1e-10))\n",
    "            \n",
    "            save_dict = {**gpt.W, **{f\"m_{k}\": gpt.m[k] for k in gpt.W}, **{f\"v_{k}\": gpt.v[k] for k in gpt.W}}\n",
    "            np.savez(weight_path, **save_dict)\n",
    "            print(f\"Step {i:5d} | Loss: {v_loss:.4f} \", flush=True)\n",
    "\n",
    "print(\"\\n--- Chatting ---\")\n",
    "while True:\n",
    "    u = input(\"\\n> \").strip()\n",
    "    if u in ['q', 'exit']: break\n",
    "    gpt.generate(u, tokenizer,  length=60, tmp=0.8)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8475967",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_games = [\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.9, 0.9, 0.9], \n",
    "    [0.8, 0.2, 0.5],\n",
    "]\n",
    "results = [0, 0, 1]\n",
    "\n",
    "def knn_predict(new_data, dataset, targets, k=3):\n",
    "    distances = []\n",
    "    for i in range(len(dataset)):\n",
    "        dist = sum((new_data[j] - dataset[i][j])**2 for j in range(len(new_data)))**0.5\n",
    "        distances.append((dist, targets[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    nearest = [d[1] for d in distances[:k]]\n",
    "    return \"Хит\" if max(set(nearest), key=nearest.count) == 0 else \"Провал\"\n",
    "\n",
    "test_game = [0.15, 0.85, 0.12]\n",
    "print(f\"Вердикт KNN: {knn_predict(test_game, old_games, results, k=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afff137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_predict(game):\n",
    "    if game[1] > 0.8:\n",
    "        if game[0] < 0.3: return \"Инди-хит\"\n",
    "        else: return \"Блокбастер\"\n",
    "    else: return \"Провал\"\n",
    "\n",
    "new_game = [0.1, 0.9, 0.5]\n",
    "print(f\"Вердикт дерева: {tree_predict(new_game)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "dataset = [[0.1, 0.8, 0.1], [0.9, 0.9, 0.9], [0.8, 0.2, 0.5]]\n",
    "targets = [1, 1, -1] \n",
    "weights = [random.uniform(-0.1, 0.1) for _ in range(3)]\n",
    "bias = 0.0\n",
    "lr = 0.01 * 2\n",
    "C = 1.0\n",
    "epochs = 1000\n",
    "l1_param = 0.005\n",
    "l2_param = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, x in enumerate(dataset):\n",
    "        condition = targets[i] * (sum(x[j] * weights[j] for j in range(3)) + bias)\n",
    "        if condition >= 1:\n",
    "            for j in range(3):\n",
    "                weights[j] -= lr * (l2_param * weights[j] + l1_param * (1 if weights[j] > 0 else -1))\n",
    "        else:\n",
    "            for j in range(3):\n",
    "                weights[j] -= lr * (l2_param * weights[j] - C * x[j] * targets[i])\n",
    "            bias += lr * C * targets[i]\n",
    "\n",
    "test_game = [0.15, 0.85, 0.12]\n",
    "result = sum(test_game[j] * weights[j] for j in range(3)) + bias\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "prob_hit = sigmoid(result)\n",
    "print(f\"SVM вердикт: {'Хит' if result > 0 else 'Провал'}\")\n",
    "print(f\"Счет (Score): {result:.2f}\")\n",
    "print(f\"Уверенность (Вероятность Хима): {prob_hit:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
