{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10534b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Deep Sovereign (3 Layers | Residual Flow)...\n",
      "\n",
      "Step 0 | Loss: 7.4992 | Grad Norm: 4.6395\n",
      "aha�ngere you didn't everonad beca\n",
      "Wecou\u0011ate three \n",
      "and T�MinisterMinisterM�yeah Oyear\t\n",
      "------------------------------\n",
      "\n",
      "Step 50 | Loss: 7.3302 | Grad Norm: 4.2421\n",
      "sedrto the ant \n",
      "The V\n",
      "Sorfromely th55fetalking �terp of Minister��ill erm �\n",
      "------------------------------\n",
      "\n",
      "Step 100 | Loss: 7.3387 | Grad Norm: 4.3608\n",
      "gigiothered to if you their Hvinot2eth�these clpecI don't thiny y mawarest�oh serving the \n",
      "------------------------------\n",
      "\n",
      "Step 150 | Loss: 7.4209 | Grad Norm: 4.6891\n",
      "�at n't Ydid distric\u001epleangqqed to youngover the times ty thinthin there ieaancldn't ldn't ldn't \n",
      "------------------------------\n",
      "\n",
      "Step 200 | Loss: 7.1532 | Grad Norm: 4.1504\n",
      "\n",
      "Yrightcayour toto�shmoroh hapemcommlik�of the edtadn't dn't te,fec the ter\n",
      "------------------------------\n",
      "\n",
      "Step 250 | Loss: 7.2415 | Grad Norm: 4.1749\n",
      "�ation�AfrMinistargot fromfe\n",
      "Oh t the at ��$don't seemfi thertimthing �with erm al \n",
      "------------------------------\n",
      "\n",
      "Step 300 | Loss: 7.3422 | Grad Norm: 4.3175\n",
      "m  warast choundmosright recoagencI've wor\u0002stalre \u0010Angby someth�ere you ghtreally \n",
      "------------------------------\n",
      "\n",
      "Step 350 | Loss: 7.1530 | Grad Norm: 4.1972\n",
      "firbus\n",
      "I whicgot  thallwaywayw  worser,from ynot�noititoo\n",
      "Mjtwt the inter\n",
      "------------------------------\n",
      "\n",
      "Step 400 | Loss: 7.1945 | Grad Norm: 4.1155\n",
      "of _tUxwhenprobbac thing which \n",
      "But 've 've ghtin whichually coursasahon the iccall w thou\n",
      "------------------------------\n",
      "\n",
      "Step 450 | Loss: 7.1561 | Grad Norm: 4.0444\n",
      " the atiw exup\n",
      "Weed to vidend hearthis icaere you morty gh�going to asm ld bl there weekdi\n",
      "------------------------------\n",
      "\n",
      "Step 500 | Loss: 7.1510 | Grad Norm: 4.1649\n",
      "mad with then hepeople frdidn't particn ant to I don't erm  ationdifereitt��gngn�it wMinistry few Y\n",
      "------------------------------\n",
      "\n",
      "Step 550 | Loss: 7.1566 | Grad Norm: 4.0234\n",
      "ZamZame was  the \n",
      "It \n",
      "It for the for the congrethendon't er\n",
      "nviall got talking about 2��timtim\n",
      "Yes surich \n",
      "------------------------------\n",
      "\n",
      "Step 600 | Loss: 7.1818 | Grad Norm: 4.0847\n",
      "as didn't �her ermghtafter werpeciplacSouth here give ain/�byaras �you know URwith with \n",
      "------------------------------\n",
      "\n",
      "Step 650 | Loss: 7.2154 | Grad Norm: 5.0331\n",
      "actually actually \n",
      "but both earmber 've 've ant \n",
      "Well es't ith�epfrad of the really heara over ��we\n",
      "------------------------------\n",
      "\n",
      "Step 700 | Loss: 7.1752 | Grad Norm: 4.2283\n",
      "g ationationationtogbut particulXent \n",
      "Hage ually alright this ninancent\n",
      "Yeahto the nice about about ofsomethhe's \n",
      "------------------------------\n",
      "\n",
      "Step 750 | Loss: 7.0713 | Grad Norm: 4.0141\n",
      "of course you know �get hopewas ��Vloobl\n",
      "And remequite 'thercanayfor I hope �erer we \n",
      "------------------------------\n",
      "\n",
      "Step 800 | Loss: 7.0777 | Grad Norm: 3.9905\n",
      "portnot got if �gites  �shar with�allnow idelyou ay ver ��ust ust ipby\n",
      "------------------------------\n",
      "\n",
      "Step 850 | Loss: 7.0298 | Grad Norm: 3.9909\n",
      "ill �particdid youis that ChErs that portuphad �somethalso of course 've got hapactually t's Chri�unw expeciti\n",
      "------------------------------\n",
      "\n",
      "Step 900 | Loss: 7.0470 | Grad Norm: 3.9843\n",
      "made ill in the I've astantZligoing to going to ellsee Amer%inistrinistrooarwe're �ermons �loa\n",
      "------------------------------\n",
      "\n",
      "Step 950 | Loss: 6.8906 | Grad Norm: 4.0550\n",
      " buwhat ant to ith �anting dif wh�ound er\n",
      "\n",
      "Oh\n",
      "lynow\n",
      "Well merLonte\n",
      "R\u0013�ellrought \n",
      "------------------------------\n",
      "\n",
      "Step 1000 | Loss: 7.0350 | Grad Norm: 3.9637\n",
      "go idnowver rosserthere\n",
      "O���isage er  ast �et et somet's I e we wuting \n",
      "------------------------------\n",
      "of courking eaer\n",
      "talking about peopan%�ent�ser\n",
      "but ele were Zambied to \n",
      "Nurthat's rightver L�me verplpart�e we ahaahaahae\n",
      "Aallere ererentsharbus\u001camfor each 3moit wank you ank you por�y�came y and  therother  of feWChurquite a ant ti the  ons dismehis much ��arare\n",
      "Ohfesfecther �acseback had spve teren't (presy si\u0006��ally Lonyearperdo wole eldeingtime\n",
      "YwhenirAmer#s  's 5tion mberida \n",
      "but m\n",
      "hat because \n",
      "Rartoo�nice ly this e\n",
      "OhLone\n",
      "Nhas 't 't �\u0014many betparly uhap\n",
      "No�did youdid youacac\n",
      "Oh \n",
      "\n",
      "Oh \n",
      "they Je we �_\n",
      "That's Robin whichdisther �hearprobimL=lea�\n",
      "I hat's out��thingthingthere �my �for the ;$k that pore\n",
      "am\u001ehas �ank saying ightfininter\n",
      "Bend mmmmouthinght\n",
      "Well be d�\n",
      "Oh\n",
      "�\n",
      "Wellmuch \n",
      "ministhat Danit�arhis  therxtend�lyic years know worI wmadlook into �rightany ank you probtoo reallumin�thiner I'ahat's ter��proclw ne their ight ]im4by our this because you knowa nethin�ll \n",
      "But ous0that's �worworwhat probtoo rearea�er\n",
      "A}come �-presyou know  the nd ool5m nexere you amams and ing that "
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. TOKENIZER (BPE) ---\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=1024):\n",
    "        self.vocab_size, self.merges, self.vocab = vocab_size, {}, {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "    def train(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        for i in range(self.vocab_size - 256):\n",
    "            stats = {}\n",
    "            for pair in zip(tokens, tokens[1:]): stats[pair] = stats.get(pair, 0) + 1\n",
    "            if not stats: break\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            self.merges[pair], self.vocab[idx] = idx, self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "            new_tokens, j = [], 0\n",
    "            while j < len(tokens):\n",
    "                if j < len(tokens)-1 and (tokens[j], tokens[j+1]) == pair:\n",
    "                    new_tokens.append(idx); j += 2\n",
    "                else: new_tokens.append(tokens[j]); j += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) >= 2:\n",
    "            stats = {p: self.merges[p] for p in zip(tokens, tokens[1:]) if p in self.merges}\n",
    "            if not stats: break\n",
    "            pair = min(stats.keys(), key=lambda p: self.merges[p])\n",
    "            idx, new_tokens, j = self.merges[pair], [], 0\n",
    "            while j < len(tokens):\n",
    "                if j < len(tokens)-1 and (tokens[j], tokens[j+1]) == pair:\n",
    "                    new_tokens.append(idx); j += 2\n",
    "                else: new_tokens.append(tokens[j]); j += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return b\"\".join(self.vocab.get(i, b\"<?>\") for i in ids).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "# --- 2. DEEP SOVEREIGN ENGINE ---\n",
    "class SovereignModel:\n",
    "    def __init__(self, v_size, emb, layers, b_size, n_experts):\n",
    "        self.v_size, self.emb, self.layers, self.b_size = v_size, emb, layers, b_size\n",
    "        self.n_experts = n_experts\n",
    "        self.dt = 0.5 \n",
    "        \n",
    "        # Global Weights\n",
    "        self.W = {\n",
    "            'E': np.random.randn(v_size, emb) * np.sqrt(2/v_size),\n",
    "            'router': np.random.randn(emb, n_experts) * 0.02\n",
    "        }\n",
    "        \n",
    "        # Layer-Specific Parameters\n",
    "        for l in range(layers):\n",
    "            self.W[f'W_kv_up_l{l}'] = np.random.randn(emb, emb * 2) * 0.05\n",
    "            for e in range(n_experts):\n",
    "                self.W[f'Wq_e{e}_l{l}'] = np.random.randn(emb, emb) * 0.02\n",
    "                self.W[f'Wo_e{e}_l{l}'] = np.random.randn(emb, emb) * 0.02\n",
    "                # Dynamic LNN Time Constants (Some fast, some slow)\n",
    "                self.W[f'tau_e{e}_l{l}'] = np.random.uniform(0.1, 2.0, (1, emb))\n",
    "\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.W.items()}\n",
    "        self.v = {k: np.zeros_like(v) for k, v in self.W.items()}\n",
    "\n",
    "    def rms_norm(self, x):\n",
    "        return x * (np.mean(x**2, axis=-1, keepdims=True) + 1e-6)**-0.5\n",
    "\n",
    "    def softmax(self, x, temp=1.0):\n",
    "        e_x = np.exp((x - np.max(x, axis=-1, keepdims=True)) / temp)\n",
    "        return e_x / (e_x.sum(axis=-1, keepdims=True) + 1e-9)\n",
    "\n",
    "    def forward(self, idx, return_cache=False):\n",
    "        h = self.rms_norm(self.W['E'][idx])\n",
    "        cache = {'h_0': h.copy()}\n",
    "        \n",
    "        for l in range(self.layers):\n",
    "            h_in = h.copy()\n",
    "            \n",
    "            # Routing\n",
    "            expert_idx = np.argmax(h @ self.W['router'], axis=-1)\n",
    "            \n",
    "            # MLA / Liquid Attention\n",
    "            kv = h @ self.W[f'W_kv_up_l{l}']\n",
    "            k, v = np.split(kv, 2, axis=-1)\n",
    "            \n",
    "            h_expert = np.zeros_like(h)\n",
    "            for e in range(self.n_experts):\n",
    "                mask = (expert_idx == e)\n",
    "                if not np.any(mask): continue\n",
    "                \n",
    "                # Context Extraction\n",
    "                q_e = h[mask] @ self.W[f'Wq_e{e}_l{l}']\n",
    "                attn = (q_e @ k[mask].T) / np.sqrt(self.emb)\n",
    "                context = (self.softmax(attn) @ v[mask]) @ self.W[f'Wo_e{e}_l{l}']\n",
    "                \n",
    "                # LNN Integration\n",
    "                dh_dt = (-h[mask] + context) / self.W[f'tau_e{e}_l{l}']\n",
    "                h_expert[mask] = h[mask] + dh_dt * self.dt\n",
    "            \n",
    "            # Residual Stream + Activation\n",
    "            h = np.tanh(h_in + h_expert)\n",
    "            cache[f'h_{l+1}'] = h.copy()\n",
    "            cache[f'idx_l{l}'] = expert_idx\n",
    "            \n",
    "        if return_cache: return h, cache\n",
    "        return h @ self.W['E'].T\n",
    "\n",
    "    def backward_step(self, x_idx, targets):\n",
    "        h_final, cache = self.forward(x_idx, return_cache=True)\n",
    "        logits = h_final @ self.W['E'].T\n",
    "        probs = self.softmax(logits)\n",
    "        \n",
    "        B, T, V = probs.shape\n",
    "        flat_probs = probs.reshape(-1, V)\n",
    "        flat_targets = targets.reshape(-1)\n",
    "        \n",
    "        # Cross-Entropy Loss\n",
    "        loss = -np.mean(np.log(flat_probs[np.arange(len(flat_targets)), flat_targets] + 1e-9))\n",
    "        \n",
    "        # Initial Gradient\n",
    "        d_logits = flat_probs.copy()\n",
    "        d_logits[np.arange(len(flat_targets)), flat_targets] -= 1\n",
    "        d_logits = d_logits.reshape(B, T, V) / (B * T)\n",
    "        \n",
    "        grads = {k: np.zeros_like(v) for k, v in self.W.items()}\n",
    "        grads['E'] = np.sum(d_logits.transpose(0, 2, 1) @ h_final, axis=0)\n",
    "        \n",
    "        # Residual Backprop through Layers\n",
    "        dh = d_logits @ self.W['E']\n",
    "        for l in reversed(range(self.layers)):\n",
    "            h_curr = cache[f'h_{l+1}']\n",
    "            h_prev = cache[f'h_{l}']\n",
    "            \n",
    "            # Update Router (Gradient Projection)\n",
    "            h_flat, dh_flat = h_prev.reshape(-1, self.emb), dh.reshape(-1, self.emb)\n",
    "            route_grad = np.sum(dh_flat * h_flat, axis=1, keepdims=True) @ np.ones((1, self.n_experts))\n",
    "            grads['router'] += h_flat.T @ route_grad * 0.001\n",
    "            \n",
    "            dh_split = np.concatenate([dh, dh], axis=-1) \n",
    "            \n",
    "            # Now the shapes match: (emb, tokens) @ (tokens, emb*2) -> (emb, emb*2)\n",
    "            grads[f'W_kv_up_l{l}'] = np.sum(h_prev.transpose(0, 2, 1) @ dh_split, axis=0)\n",
    "            # Chain Rule through Residual + Tanh\n",
    "            dh = dh + (dh @ self.W[f'W_kv_up_l{l}'][:, :self.emb].T)\n",
    "            dh = dh * (1 - h_curr**2) \n",
    "            \n",
    "        return loss, grads\n",
    "\n",
    "    def update(self, grads, lr):\n",
    "        rho = 0.02\n",
    "        for k in self.W:\n",
    "            self.m[k] = 0.9 * self.m[k] + 0.1 * grads[k]\n",
    "            self.v[k] = 0.99 * self.v[k] + 0.01 * (grads[k]**2)\n",
    "            denom = np.sqrt(self.v[k]) + 1e-5\n",
    "            self.W[k] -= lr * np.clip(self.m[k] / denom, -rho, rho)\n",
    "\n",
    "    def generate(self, prompt, tok, length=100, temp=1.1):\n",
    "        ids = tok.encode(prompt.lower())\n",
    "        for _ in range(length):\n",
    "            context = np.array(ids[-self.b_size:]).reshape(1, -1)\n",
    "            logits = self.forward(context)[0, -1, :]\n",
    "            \n",
    "            # Dynamic Repetition Penalty\n",
    "            for prev_id in set(ids[-15:]): logits[prev_id] -= 1.5\n",
    "            \n",
    "            p = self.softmax(logits, temp=temp)\n",
    "            next_id = np.random.choice(len(p), p=p)\n",
    "            ids.append(next_id)\n",
    "            print(tok.decode([next_id]), end=\"\", flush=True)\n",
    "\n",
    "# --- 3. RUNTIME ---\n",
    "tokenizer = BPETokenizer(1024)\n",
    "model = SovereignModel(1024, 384, 3, 128, 8) \n",
    "\n",
    "try:\n",
    "    with open('C:\\\\Users\\\\user\\\\Downloads\\\\BNCCorpus.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()[:40000]\n",
    "    tokenizer.train(text)\n",
    "    data = np.array(tokenizer.encode(text))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training Deep Sovereign (3 Layers | Residual Flow)...\")\n",
    "for i in range(1001):\n",
    "    lr = 0.001 * (0.5 * (1 + np.cos(np.pi * i / 1000)))\n",
    "    ix = np.random.randint(0, len(data) - 64 - 1)\n",
    "    xb, yb = data[ix:ix+64].reshape(1, -1), data[ix+1:ix+64+1].reshape(1, -1)\n",
    "    \n",
    "    loss, grads = model.backward_step(xb, yb)\n",
    "    \n",
    "    # Clip Gradients\n",
    "    total_norm = np.sqrt(sum(np.sum(g**2) for g in grads.values()))\n",
    "    if total_norm > 1.0:\n",
    "        for k in grads: grads[k] /= (total_norm + 1e-6)\n",
    "        \n",
    "    model.update(grads, lr)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(f\"\\nStep {i} | Loss: {loss:.4f} | Grad Norm: {total_norm:.4f}\")\n",
    "        model.generate(\"The \", tokenizer, length=25)\n",
    "        print(\"\\n\" + \"-\"*30)\n",
    "\n",
    "while True:\n",
    "    u = input(\"\\nSovereign> \").strip()\n",
    "    if u.lower() in ['q', 'exit']: break\n",
    "    model.generate(u, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "\n",
    "# --- TOKENIZER (BPE) ---\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=512):\n",
    "        self.vocab_size, self.merges, self.vocab = vocab_size, {}, {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "    def train(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        for i in range(self.vocab_size - 256):\n",
    "\n",
    "            stats = {}\n",
    "            for pair in zip(tokens, tokens[1:]): stats[pair] = stats.get(pair, 0) + 1\n",
    "            if not stats: break\n",
    "\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            self.merges[pair], self.vocab[idx] = idx, self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "            new_tokens, j = [], 0\n",
    "\n",
    "            while j < len(tokens):\n",
    "                if j < len(tokens)-1 and (tokens[j], tokens[j+1]) == pair:\n",
    "                    new_tokens.append(idx); j += 2\n",
    "                else: new_tokens.append(tokens[j]); j += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) >= 2:\n",
    "\n",
    "            stats = {p: self.merges[p] for p in zip(tokens, tokens[1:]) if p in self.merges}\n",
    "            if not stats: break\n",
    "\n",
    "            pair = min(stats.keys(), key=lambda p: self.merges[p])\n",
    "            idx, new_tokens, j = self.merges[pair], [], 0\n",
    "\n",
    "            while j < len(tokens):\n",
    "                if j < len(tokens)-1 and (tokens[j], tokens[j+1]) == pair:\n",
    "                    new_tokens.append(idx); j += 2\n",
    "                else: new_tokens.append(tokens[j]); j += 1\n",
    "            tokens = new_tokens\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return b\"\".join(self.vocab[idx] for idx in ids).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "# --- MODEL ---\n",
    "class GPT:\n",
    "    def __init__(self, v_size, emb, h, groups, layers, b_size):\n",
    "        self.v_size, self.emb, self.h, self.layers, self.b_size = v_size, emb, h, layers, b_size\n",
    "        self.groups, self.head_dim = groups, emb // h\n",
    "        self.heads_per_group = h // groups \n",
    "\n",
    "        self.mask = np.triu(np.ones((b_size, b_size)), 1).astype(bool)\n",
    "        inv = 1.0 / (10000 ** (np.arange(0, self.head_dim, 2) / self.head_dim))\n",
    "        freqs = np.outer(np.arange(b_size), inv)\n",
    "        self.cos_pre, self.sin_pre = np.cos(freqs).astype(np.float16), np.sin(freqs).astype(np.float16)\n",
    "\n",
    "        self.W, self.kv_cache = {}, [None] * layers\n",
    "        self.init_w()\n",
    "        self.t = 0\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.W.items()}\n",
    "        self.v = {k: np.zeros_like(v) for k, v in self.W.items()}\n",
    "\n",
    "    def init_w(self):\n",
    "        self.W['E'] = (np.random.randn(self.v_size, self.emb) * 0.02).astype(np.float16)\n",
    "        for l in range(self.layers):\n",
    "            self.W[f'g1{l}'] = np.ones((1,1,self.emb), dtype=np.float16)\n",
    "            self.W[f'g2{l}'] = np.ones((1,1,self.emb), dtype=np.float16)\n",
    "\n",
    "            std = np.sqrt(2.0 / (2 * self.emb))\n",
    "            self.W[f'Wq{l}'] = (np.random.randn(self.emb, self.emb) * std).astype(np.float16)\n",
    "            self.W[f'Wk{l}'] = (np.random.randn(self.emb, self.groups * self.head_dim) * std).astype(np.float16)\n",
    "            self.W[f'Wv{l}'] = (np.random.randn(self.emb, self.groups * self.head_dim) * std).astype(np.float16)\n",
    "            self.W[f'Wo{l}'] = (np.random.randn(self.emb, self.emb) * std).astype(np.float16)\n",
    "            \n",
    "            hidden = int(8/3 * self.emb)\n",
    "            self.W[f'Wf1{l}'] = (np.random.randn(self.emb, hidden) * np.sqrt(2/self.emb)).astype(np.float16)\n",
    "            self.W[f'Wf2{l}'] = (np.random.randn(hidden, self.emb) * np.sqrt(2/hidden)).astype(np.float16)\n",
    "            self.W[f'Wf3{l}'] = (np.random.randn(self.emb, hidden) * np.sqrt(2/self.emb)).astype(np.float16)\n",
    "\n",
    "    def rms_norm(self, x, g, out=None):\n",
    "        if out is None: out = np.empty_like(x)\n",
    "        msq = np.mean(x.astype(np.float32)**2, axis=-1, keepdims=True)\n",
    "        out[:] = (g * (x.astype(np.float32) * (msq + 1e-6)**-0.5)).astype(np.float16)\n",
    "        return out.astype(np.float16)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x_32 = x.astype(np.float32)\n",
    "        e_x = np.exp(x_32 - np.max(x_32, axis=-1, keepdims=True))\n",
    "        out = e_x / (e_x.sum(axis=-1, keepdims=True) + 1e-10)\n",
    "        return out.astype(np.float16)\n",
    "\n",
    "    def apply_rope(self, x, pos_idx, rev=False):\n",
    "        c, s = self.cos_pre[pos_idx], self.sin_pre[pos_idx]\n",
    "        if rev: s = -s\n",
    "        x1, x2 = x[..., 0::2], x[..., 1::2]\n",
    "        out = np.empty_like(x)\n",
    "        out[..., 0::2] = x1 * c - x2 * s\n",
    "        out[..., 1::2] = x1 * s + x2 * c\n",
    "        return out\n",
    "\n",
    "    def get_batch(self, data, batch_size):\n",
    "        ix = np.random.randint(0, len(data) - self.b_size, batch_size)\n",
    "        x = np.empty((batch_size, self.b_size), dtype=np.int32)\n",
    "        y = np.empty((batch_size, self.b_size), dtype=np.int32)\n",
    "        for i, idx in enumerate(ix):\n",
    "            x[i] = data[idx : idx + self.b_size]\n",
    "            y[i] = data[idx + 1 : idx + self.b_size + 1]\n",
    "        return x, y\n",
    "\n",
    "    def forward(self, idx, start_pos=0, use_cache=False):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Transform discrete Token IDs into continuous 256-dim vectors\n",
    "        # Shape: (B, T) -> (B, T, 256)\n",
    "        x = self.W['E'][idx].astype(np.float16)\n",
    "\n",
    "        pos_idx = np.arange(start_pos, start_pos + T)\n",
    "        cache = {'idx': idx, 'x_0': x.copy()} \n",
    "        \n",
    "        for l in range(self.layers):\n",
    "            ln1 = self.rms_norm(x, self.W[f'g1{l}'])\n",
    "\n",
    "            # Project normalized input into Query, Key, and Value spaces\n",
    "            # Q: (B, T, 256) @ (256, 256) -> (B, 8, T, 32) [Split into 8 heads]\n",
    "            # K/V: (B, T, 256) @ (256, 64) -> (B, 2, T, 32) [Split into 2 groups\n",
    "            q = (ln1 @ self.W[f'Wq{l}']).reshape(B, T, self.h, self.head_dim).transpose(0, 2, 1, 3).astype(np.float16)\n",
    "            k = (ln1 @ self.W[f'Wk{l}']).reshape(B, T, self.groups, self.head_dim).transpose(0, 2, 1, 3).astype(np.float16)\n",
    "            v = (ln1 @ self.W[f'Wv{l}']).reshape(B, T, self.groups, self.head_dim).transpose(0, 2, 1, 3).astype(np.float16)\n",
    "\n",
    "            q, k = self.apply_rope(q, pos_idx), self.apply_rope(k, pos_idx)\n",
    "            \n",
    "            if use_cache:\n",
    "                if self.kv_cache[l] is None or start_pos == 0: self.kv_cache[l] = (k, v)\n",
    "                else:\n",
    "                    k = np.concatenate([self.kv_cache[l][0], k], axis=2)\n",
    "                    v = np.concatenate([self.kv_cache[l][1], v], axis=2)\n",
    "                    self.kv_cache[l] = (k, v)\n",
    "            \n",
    "            # Expand Keys to match the number of Query heads (GQA logic)\n",
    "            # Shape: (B, 2, T, 32) -> (B, 8, T, 32)\n",
    "            k_rep = np.repeat(k, self.heads_per_group, axis=1)\n",
    "            v_rep = np.repeat(v, self.heads_per_group, axis=1)\n",
    "            \n",
    "            # Compute alignment scores: \"How much does word T look at word T-n?\"\n",
    "            # (B, 8, T, 32) @ (B, 8, 32, T) -> (B, 8, T, T)\n",
    "            sc = (q @ k_rep.transpose(0, 1, 3, 2)) / np.sqrt(self.head_dim)\n",
    "            if not use_cache: sc[:, :, self.mask[:T, :T]] = -65500\n",
    "            \n",
    "            # Convert scores to probabilities (Softmax)\n",
    "            # Result: (B, 8, T, T) summing to 1.0 across the last dimension\n",
    "            sc_32 = sc.astype(np.float32)\n",
    "            e_x = np.exp(sc_32 - np.max(sc_32, axis=-1, keepdims=True))\n",
    "            attn = (e_x / (e_x.sum(axis=-1, keepdims=True) + 1e-10)).astype(np.float16)\n",
    "            \n",
    "            # Use attention weights to mix the \"Content\" (Values)\n",
    "            # Input: (B, h, T, T) @ (B, h, T, head_dim) -> Output: (B, h, T, head_dim)\n",
    "            out_att = (attn @ v_rep).transpose(0, 2, 1, 3).reshape(B, T, self.emb)\n",
    "\n",
    "            # Project mixed content back to the residual highway and add it\n",
    "            # (B, T, self.emb) + (B, T, self.emb) -> (B, T, self.emb)\n",
    "            x += out_att @ self.W[f'Wo{l}']\n",
    "            \n",
    "            # Normalize the highway (Residual Stream)\n",
    "            # Input: (B, T, 256) -> Output: (B, T, 256)\n",
    "            ln2 = self.rms_norm(x, self.W[f'g2{l}'])\n",
    "\n",
    "            # Project to Hidden Space (Expansion)\n",
    "            # (B, T, 256) @ (256, 682) -> (B, T, 682)\n",
    "            gate = ln2 @ self.W[f'Wf1{l}']\n",
    "            up = ln2 @ self.W[f'Wf3{l}']\n",
    "\n",
    "            # Apply Activation (Swish)\n",
    "            # Shape remains (B, T, 682)\n",
    "            gate_32 = gate.astype(np.float32)\n",
    "            act_gate = (gate_32 / (1.0 + np.exp(-gate_32))).astype(np.float16)\n",
    "\n",
    "            # Element-wise multiply and Shrink back to Embedding size\n",
    "            # (B, T, 682) * (B, T, 682) -> (B, T, 682)\n",
    "            # (B, T, 682) @ (682, 256) -> (B, T, 256)\n",
    "            x += (act_gate * up) @ self.W[f'Wf2{l}']\n",
    "            cache[f'l{l}'] = (ln1, q, k, v, attn, out_att, ln2, gate, up, act_gate)\n",
    "\n",
    "        cache['x_final'] = x\n",
    "        return x @ self.W['E'].T, cache\n",
    "\n",
    "    def backward(self, yb, logits, cache):\n",
    "        B, T = yb.shape\n",
    "        grads = {k: np.zeros_like(v, dtype=np.float32) for k, v in self.W.items()}\n",
    "        pp_sum = np.exp(logits.astype(np.float32) - np.max(logits.astype(np.float32), axis=-1, keepdims=True))\n",
    "        probs = pp_sum / (pp_sum.sum(axis=-1, keepdims=True) + 1e-10)\n",
    "        probs[np.arange(B)[:, None], np.arange(T), yb] -= 1\n",
    "\n",
    "        # Calculate Cross-Entropy Loss gradient (Probs - Ground Truth)\n",
    "        # Shape: (B, T, Vocab)\n",
    "        dl = (probs / (B * T)).astype(np.float16)\n",
    "        \n",
    "        # Calculate error for output projection\n",
    "        # (B, T, vocab) @ (B, T, 256) -> (Vocab, 256)\n",
    "        grads['E'] += (dl.transpose(0, 2, 1) @ cache['x_final']).sum(0)\n",
    "\n",
    "        # Backprop error into the residual stream 'dx'\n",
    "        # (B, T, Vocab) @ (Vocab, Emb) -> (B, T, Emb)\n",
    "        dx = (dl @ self.W['E']).astype(np.float16)\n",
    "        pos_idx = np.arange(T)\n",
    "\n",
    "        for l in reversed(range(self.layers)):\n",
    "            ln1, q, k, v, attn, out_att, ln2, gate, up, act_gate = cache[f'l{l}']\n",
    "\n",
    "            # Move error through the SwiGLU bottleneck\n",
    "            # (B, T, 256) @ (256, 682) -> (B, T, 682)\n",
    "            df2 = dx @ self.W[f'Wf2{l}'].T\n",
    "\n",
    "            # Gradient for Wf2: Input (act_gate) @ Error (dx)\n",
    "            # (Hidden, B*T) @ (B*T, Emb) -> (Hidden, Emb)\n",
    "            grads[f'Wf2{l}'] += act_gate.reshape(-1, act_gate.shape[-1]).T @ dx.reshape(-1, self.emb)\n",
    "            \n",
    "            sig = 1 / (1 + np.exp(-gate.astype(np.float32)))\n",
    "            dgate32 = (df2.astype(np.float32) * up) * (sig * (1 + gate.astype(np.float32) * (1 - sig)))\n",
    "            dgate = dgate32.astype(np.float16)\n",
    "            dup = (df2 * act_gate).astype(np.float16)\n",
    "\n",
    "            # Gradient for Weight f3 (Up projection)\n",
    "            # (256, B*T) @ (B*T, 682) -> (256, 682)\n",
    "            grads[f'Wf3{l}'] += ln2.reshape(-1, self.emb).T @ dup.reshape(-1, dup.shape[-1])\n",
    "            grads[f'Wf1{l}'] += ln2.reshape(-1, self.emb).T @ dgate.reshape(-1, dgate.shape[-1])\n",
    "            \n",
    "            # Update dx with the error flowing through the MLP back to the main highway\n",
    "            # (B, T, Hidden) @ (Hidden, Emb) -> (B, T, Emb)\n",
    "            dx += (dgate @ self.W[f'Wf1{l}'].T) + (dup @ self.W[f'Wf3{l}'].T)\n",
    "            \n",
    "            grads[f'Wo{l}'] += out_att.reshape(-1, self.emb).T @ dx.reshape(-1, self.emb)\n",
    "            \n",
    "            # Error for the attention mechanism result\n",
    "            # (B, T, Emb) @ (Emb, Emb) -> (B, 8, T, 32)\n",
    "            dout = (dx @ self.W[f'Wo{l}'].T).reshape(B, T, self.h, self.head_dim).transpose(0, 2, 1, 3)\n",
    "            \n",
    "            # Error for V: Sum heads back into groups\n",
    "            # (B, 8, T, T).T @ (B, 8, T, 32) -> (B, 2, T, 32)\n",
    "            dv = (attn.transpose(0, 1, 3, 2) @ dout).reshape(B, self.groups, self.heads_per_group, T, self.head_dim).sum(2)\n",
    "            \n",
    "            # Error for Attention Scores (Softmax derivative)\n",
    "            da = (dout @ np.repeat(v, self.heads_per_group, axis=1).transpose(0, 1, 3, 2))\n",
    "            da_f32 = da.astype(np.float32)\n",
    "            attn_f32 = attn.astype(np.float32)\n",
    "            ds = attn_f32 * (da_f32 - (attn_f32 * da_f32).sum(-1, keepdims=True))\n",
    "            ds = ds.astype(np.float16)\n",
    "\n",
    "            # Error for Q and K (Untwist RoPE first)\n",
    "            dq = self.apply_rope(ds @ np.repeat(k, self.heads_per_group, axis=1), pos_idx, rev=True) / np.sqrt(self.head_dim)\n",
    "            dk = self.apply_rope(ds.transpose(0, 1, 3, 2) @ q, pos_idx, rev=True).reshape(B, self.groups, self.heads_per_group, T, self.head_dim).sum(2) / np.sqrt(self.head_dim)\n",
    "            \n",
    "            # Backprop to Q, K, V Projection weights\n",
    "            # (Emb, B*T) @ (B*T, Head_Dim) -> (Emb, Emb)\n",
    "            grads[f'Wq{l}'] += ln1.reshape(-1, self.emb).T @ dq.transpose(0, 2, 1, 3).reshape(-1, self.emb)\n",
    "            grads[f'Wk{l}'] += ln1.reshape(-1, self.emb).T @ dk.transpose(0, 2, 1, 3).reshape(-1, self.groups*self.head_dim)\n",
    "            grads[f'Wv{l}'] += ln1.reshape(-1, self.emb).T @ dv.transpose(0, 2, 1, 3).reshape(-1, self.groups*self.head_dim)\n",
    "            \n",
    "            dx += (dq.transpose(0, 2, 1, 3).reshape(-1, self.emb) @ self.W[f'Wq{l}'].T).reshape(B, T, self.emb)\n",
    "        \n",
    "        np.add.at(grads['E'], cache['idx'], dx)\n",
    "        return grads\n",
    "\n",
    "    def update(self, grads, step, lr_max, warmup, wd):\n",
    "        self.t += 1\n",
    "        lr = lr_max * min(1.0, step/warmup) * max(0.1, 1.0 - (step-warmup)/4000)\n",
    "        for k in self.W:\n",
    "            g = np.clip(grads[k], -1.0, 1.0)\n",
    "\n",
    "            self.m[k] = (0.9 * self.m[k].astype(np.float32) + 0.1 * g)\n",
    "            self.v[k] = (0.999 * self.v[k].astype(np.float32) + 0.001 * (g**2))\n",
    "\n",
    "            mh = self.m[k] / (1 - 0.9**self.t)\n",
    "            vh = self.v[k] / (1 - 0.999**self.t)\n",
    "            \n",
    "            update_val = lr * (mh / (np.sqrt(vh) + 1e-8) + wd * self.W[k].astype(np.float32))\n",
    "            self.W[k] = (self.W[k].astype(np.float32) - update_val).astype(np.float16)\n",
    "        \n",
    "        if self.t % 25 == 0:\n",
    "                for k, g in grads.items():\n",
    "                    print(f\"{k} | Mean: {np.mean(g):.6f} | Std: {np.std(g):.6f}\")\n",
    "\n",
    "    def generate(self, prompt, tok, length=30, tmp=0.7, k=40, p=0.9):\n",
    "        ids = tok.encode(prompt)\n",
    "        self.kv_cache = [None] * self.layers\n",
    "        curr_ids = np.array(ids).reshape(1, -1)\n",
    "\n",
    "        for _ in range(length):\n",
    "            logits, _ = self.forward(curr_ids, start_pos=len(ids)-curr_ids.shape[1], use_cache=True)\n",
    "            logits = logits[0, -1, :].astype(np.float32) / (tmp + 1e-10)\n",
    "\n",
    "            if k > 0: logits[logits < np.partition(logits, -k)[-k]] = -1e4\n",
    "            probs = np.exp(logits - np.max(logits))\n",
    "            probs /= (probs.sum() + 1e-10)\n",
    "\n",
    "            if p < 1.0:\n",
    "                si = np.argsort(probs)[::-1]; sp = probs[si]; cp = np.cumsum(sp)\n",
    "                ir = cp > p; ir[1:] = ir[:-1].copy(); ir[0] = False\n",
    "                probs[si[ir]] = 0; probs /= (probs.sum() + 1e-10)\n",
    "                \n",
    "            next_id = np.random.choice(len(probs), p=probs)\n",
    "            ids.append(next_id); curr_ids = np.array([[next_id]])\n",
    "            print(tok.decode([next_id]), end=\"\", flush=True)\n",
    "        return ids\n",
    "\n",
    "# --- CONFIG ---\n",
    "path = \"C:\\\\Users\\\\user\\\\Downloads\\\\BNCCorpus.txt\" \n",
    "weight_path = \"C:\\\\Users\\\\user\\\\Downloads\\\\model_weights.npz\"\n",
    "token_path = \"C:\\\\Users\\\\user\\\\Downloads\\\\tokenized_data.npy\"\n",
    "vocab_size, block_size, embed, groups, heads, n_layers = 384, 128, 64, 2, 4, 3\n",
    "lr_max, batch_size, warmup, accum_steps, wd = 0.0001, 4, 200, 1, 0.1\n",
    "\n",
    "# --- EXECUTION ---\n",
    "try:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f: text = f.read().lower()[:150000]\n",
    "except: print('No data...'); sys.exit()\n",
    "\n",
    "tokenizer = BPETokenizer(vocab_size)\n",
    "if os.path.exists(token_path):\n",
    "    token_data = np.load(token_path)\n",
    "    print(\"Loaded pre-tokenized data.\")\n",
    "else:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f: \n",
    "        text = f.read().lower()[:150000]\n",
    "    print(\"Tokenizing text (this may take a few minutes once)...\")\n",
    "    token_data = np.array(tokenizer.encode(text), dtype=np.uint16)\n",
    "    np.save(token_path, token_data)\n",
    "    print(\"Tokens saved.\")\n",
    "\n",
    "n = int(0.9 * len(token_data))\n",
    "train_data, val_data = token_data[:n], token_data[n:]\n",
    "\n",
    "gpt = GPT(vocab_size, embed, heads, groups, n_layers, block_size)\n",
    "if os.path.exists(weight_path):\n",
    "    cp = np.load(weight_path)\n",
    "\n",
    "    for k in gpt.W:\n",
    "        if k in cp: gpt.W[k] = cp[k]\n",
    "        if f\"m_{k}\" in cp: gpt.m[k], gpt.v[k] = cp[f\"m_{k}\"], cp[f\"v_{k}\"]\n",
    "    print(\"Existing weights loaded.\")\n",
    "    mode = input(\"\\n[T]rain further or [C]hat only? \").lower()\n",
    "\n",
    "else:\n",
    "    print(\"No weights found. Starting fresh training...\")\n",
    "    mode = 't'\n",
    "\n",
    "if mode == 't':\n",
    "    token_data = np.array(tokenizer.encode(text), dtype=np.uint16)\n",
    "    n = int(0.9 * len(token_data))\n",
    "    train_data, val_data = token_data[:n], token_data[n:]\n",
    "    acc_grads = {k: np.zeros_like(v, dtype=np.float32) for k, v in gpt.W.items()}\n",
    "    \n",
    "    for i in range(401):\n",
    "        xb, yb = gpt.get_batch(train_data, batch_size)\n",
    "        logits, cache = gpt.forward(xb)\n",
    "        grads = gpt.backward(yb, logits, cache)\n",
    "        for k in grads: acc_grads[k] += np.clip(grads[k], -1.0, 1.0)\n",
    "        del cache  \n",
    "        del grads\n",
    "\n",
    "        if i % 1 == 0: \n",
    "            print(f\"Iteration {i} started...\", end='\\r')\n",
    "        \n",
    "        if (i + 1) % accum_steps == 0:\n",
    "            for k in acc_grads: acc_grads[k] /= accum_steps\n",
    "            gpt.update(acc_grads, i // accum_steps, lr_max, warmup, wd)\n",
    "            for k in acc_grads: acc_grads[k].fill(0)\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            xv, yv = gpt.get_batch(val_data, batch_size)\n",
    "            v_logits, _ = gpt.forward(xv)\n",
    "            v_probs = gpt.softmax(v_logits)\n",
    "            v_loss = -np.mean(np.log(v_probs[np.arange(batch_size)[:,None], np.arange(block_size), yv] + 1e-10))\n",
    "            \n",
    "            save_dict = {**gpt.W, **{f\"m_{k}\": gpt.m[k] for k in gpt.W}, **{f\"v_{k}\": gpt.v[k] for k in gpt.W}}\n",
    "            np.savez(weight_path, **save_dict)\n",
    "            print(f\"Step {i:5d} | Loss: {v_loss:.4f} \", flush=True)\n",
    "\n",
    "print(\"\\n--- Chatting ---\")\n",
    "while True:\n",
    "    u = input(\"\\n> \").strip()\n",
    "    if u in ['q', 'exit']: break\n",
    "    gpt.generate(u, tokenizer,  length=60, tmp=0.8)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8475967",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_games = [\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.9, 0.9, 0.9], \n",
    "    [0.8, 0.2, 0.5],\n",
    "]\n",
    "results = [0, 0, 1]\n",
    "\n",
    "def knn_predict(new_data, dataset, targets, k=3):\n",
    "    distances = []\n",
    "    for i in range(len(dataset)):\n",
    "        dist = sum((new_data[j] - dataset[i][j])**2 for j in range(len(new_data)))**0.5\n",
    "        distances.append((dist, targets[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    nearest = [d[1] for d in distances[:k]]\n",
    "    return \"Хит\" if max(set(nearest), key=nearest.count) == 0 else \"Провал\"\n",
    "\n",
    "test_game = [0.15, 0.85, 0.12]\n",
    "print(f\"Вердикт KNN: {knn_predict(test_game, old_games, results, k=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afff137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_predict(game):\n",
    "    if game[1] > 0.8:\n",
    "        if game[0] < 0.3: return \"Инди-хит\"\n",
    "        else: return \"Блокбастер\"\n",
    "    else: return \"Провал\"\n",
    "\n",
    "new_game = [0.1, 0.9, 0.5]\n",
    "print(f\"Вердикт дерева: {tree_predict(new_game)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "dataset = [[0.1, 0.8, 0.1], [0.9, 0.9, 0.9], [0.8, 0.2, 0.5]]\n",
    "targets = [1, 1, -1] \n",
    "weights = [random.uniform(-0.1, 0.1) for _ in range(3)]\n",
    "bias = 0.0\n",
    "lr = 0.01 * 2\n",
    "C = 1.0\n",
    "epochs = 1000\n",
    "l1_param = 0.005\n",
    "l2_param = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, x in enumerate(dataset):\n",
    "        condition = targets[i] * (sum(x[j] * weights[j] for j in range(3)) + bias)\n",
    "        if condition >= 1:\n",
    "            for j in range(3):\n",
    "                weights[j] -= lr * (l2_param * weights[j] + l1_param * (1 if weights[j] > 0 else -1))\n",
    "        else:\n",
    "            for j in range(3):\n",
    "                weights[j] -= lr * (l2_param * weights[j] - C * x[j] * targets[i])\n",
    "            bias += lr * C * targets[i]\n",
    "\n",
    "test_game = [0.15, 0.85, 0.12]\n",
    "result = sum(test_game[j] * weights[j] for j in range(3)) + bias\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "prob_hit = sigmoid(result)\n",
    "print(f\"SVM вердикт: {'Хит' if result > 0 else 'Провал'}\")\n",
    "print(f\"Счет (Score): {result:.2f}\")\n",
    "print(f\"Уверенность (Вероятность Хима): {prob_hit:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
